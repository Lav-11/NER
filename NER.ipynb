{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install pydantic\n",
    "!pip install seqeval\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(0)\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Print a sample\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        if(pred_label < 0 or pred_label > 8):\n",
    "            print(f\"Token: {tokens[i]}, Predicted Label: {pred_label}, True Label: {true_labels[i]}\")\n",
    "        #assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv_vanilla(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    if (len(pred_labels) == 0 and len(true_labels) == 0):\n",
    "        return\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens)) if pred_labels[i] != 0 or true_labels[i] != 0]\n",
    "    # Remove duplicates\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 300 random elements from the test set\n",
    "sampled_test_data = random.sample(list(test_data), 300)\n",
    "\n",
    "# Print the first few samples to verify\n",
    "for i, sample in enumerate(sampled_test_data[:5]):  # Display the first 5 samples\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(sample)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'\n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'\n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data/vanilla_test_300_gemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "category_to_index = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, \n",
    "    'I-PER': 2, \n",
    "    'B-ORG': 3, \n",
    "    'I-ORG': 4, \n",
    "    'B-LOC': 5, \n",
    "    'I-LOC': 6, \n",
    "    'B-MISC': 7, \n",
    "    'I-MISC': 8\n",
    "    }\n",
    "\n",
    "# Step 2: Read the CSV and convert predictions and true labels\n",
    "true_seqs = []\n",
    "pred_seqs = []\n",
    "current_true = []\n",
    "current_pred = []\n",
    "\n",
    "with open('data/no_bio_test.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        token = row['token']\n",
    "        # true_label = index_to_label[int(row['true'])]\n",
    "        # pred_label = index_to_label[int(row['pred'])]\n",
    "        true_label = label_mapping[int(row['true'])]\n",
    "        pred_label = label_mapping[int(row['pred'])]\n",
    "\n",
    "        current_true.append(true_label)\n",
    "        current_pred.append(pred_label)\n",
    "\n",
    "    true_seqs.append(current_true)\n",
    "    pred_seqs.append(current_pred)\n",
    "\n",
    "# Step 3: Compute metrics\n",
    "print(\"Precision:\", precision_score(true_seqs, pred_seqs))\n",
    "print(\"Recall:\", recall_score(true_seqs, pred_seqs))\n",
    "print(\"F1 Score:\", f1_score(true_seqs, pred_seqs))\n",
    "\n",
    "# Optional detailed report\n",
    "print(\"\\nDetailed classification report:\\n\")\n",
    "print(classification_report(true_seqs, pred_seqs))\n",
    "# Precision: 0.4524647887323944\n",
    "# Recall: 0.5320910973084886\n",
    "# F1 Score: 0.48905803996194097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposed-QA\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "\n",
    "def save_to_csv_qa(pred_labels, true_labels_indices, tokens, filename):\n",
    "    data = []\n",
    "    \n",
    "    # Convert true_labels_indices to a dictionary mapping tokens to their true labels\n",
    "    true_label_dict = {}\n",
    "    for token, label_idx in zip(tokens, true_labels_indices):\n",
    "        # Ensure we're using a valid label from label_mapping\n",
    "        if label_idx in label_mapping:\n",
    "            true_label_dict[token] = label_mapping[label_idx]\n",
    "        else:\n",
    "            print(f\"Warning: Unknown label index {label_idx} for token '{token}', defaulting to 'O'\")\n",
    "            true_label_dict[token] = 'O'\n",
    "    \n",
    "    # Print for debugging\n",
    "    # print(\"True label dictionary:\", true_label_dict)\n",
    "    # print(\"Predicted labels:\", pred_labels)\n",
    "    \n",
    "    # Process each predicted label and their tokens\n",
    "    for pred_label, tokens_str in pred_labels.items():\n",
    "        # Skip invalid labels\n",
    "        if pred_label == 'None' or pred_label not in category_to_index:\n",
    "            print(f\"Warning: Skipping invalid label '{pred_label}'\")\n",
    "            continue\n",
    "            \n",
    "        # Split by commas to get individual tokens\n",
    "        token_list = [t.strip() for t in tokens_str.split(',')]\n",
    "        \n",
    "        for token in token_list:\n",
    "            if not token:  # Skip empty tokens\n",
    "                continue\n",
    "                \n",
    "            # Check if token exists in original tokens\n",
    "            if token in tokens:\n",
    "                # Direct match with a single token\n",
    "                true_label = true_label_dict.get(token, 'O')\n",
    "                data.append([token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "            else:\n",
    "                # This could be a composite token or doesn't exist in our original tokens\n",
    "                # Try to match with individual tokens from the original list\n",
    "                matched_tokens = []\n",
    "                for orig_token in tokens:\n",
    "                    # Find approximate matches\n",
    "                    if orig_token.lower() in token.lower() or token.lower() in orig_token.lower():\n",
    "                        true_label = true_label_dict.get(orig_token, 'O')\n",
    "                        data.append([orig_token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "                        matched_tokens.append(orig_token)\n",
    "                \n",
    "                # If we still couldn't match it to any original tokens\n",
    "                if not matched_tokens:\n",
    "                    # Only add unknown tokens if they look reasonable (not empty, not \"None\", etc.)\n",
    "                    if token != \"None\" and len(token) > 1:\n",
    "                        print(f\"Warning: Could not match predicted token '{token}' to original tokens\")\n",
    "                        # We'll add it with 'O' as the true label since we can't find it\n",
    "                        data.append([token, category_to_index[pred_label], category_to_index['O']])\n",
    "    \n",
    "    # Process any tokens that weren't in the predictions but have true labels\n",
    "    for token, true_label in true_label_dict.items():\n",
    "        if true_label != 'O':  # Only include non-O labels\n",
    "            # Check if this token was already processed\n",
    "            token_processed = any(entry[0] == token for entry in data)\n",
    "            if not token_processed:\n",
    "                data.append([token, category_to_index['O'], category_to_index[true_label]])\n",
    "    \n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    #print(f\"Data written to {filename}\")\n",
    "\n",
    "# Function to parse model responses and extract predicted labels\n",
    "def parse_model_response(response_text):\n",
    "    results = {}\n",
    "    \n",
    "    if response_text == 'None':\n",
    "        return results\n",
    "        \n",
    "    # Split by & to get multiple label groups\n",
    "    answers = response_text.split('&')\n",
    "    answers = [answer.strip() for answer in answers]\n",
    "    \n",
    "    for answer in answers:\n",
    "        answer = answer.strip()\n",
    "        \n",
    "        # Skip empty answers or explicit \"None\" answers\n",
    "        if not answer or answer == 'None':\n",
    "            continue\n",
    "            \n",
    "        # Check if we have a proper format with colon\n",
    "        if ':' in answer:\n",
    "            parts = answer.split(':', 1)  # Split on first colon only\n",
    "            label = parts[0].strip()\n",
    "            \n",
    "            # Validate the label before adding\n",
    "            if label not in category_to_index:\n",
    "                print(f\"Warning: Skipping invalid label format: '{label}'\")\n",
    "                continue\n",
    "                \n",
    "            # Extract entities (everything after the colon)\n",
    "            entities = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            \n",
    "            # Only add if we have non-empty entities\n",
    "            if entities:\n",
    "                results[label] = entities\n",
    "        else:\n",
    "            print(f\"Warning: Invalid format in answer: '{answer}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "label_groups = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')  # or gemma-3-27b-it\n",
    "#model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    results = {}\n",
    "    \n",
    "    chat = model.start_chat()\n",
    "    # Send general instructions only at the beginning of the conversation\n",
    "    intro_message = (\n",
    "        f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "        \n",
    "        The sentence is: '{tokens}'.\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "        Remember that a token can only be classified once.\"\"\"\n",
    "    )\n",
    "    # Send message to the chat\n",
    "    try:\n",
    "        chat.send_message(intro_message)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        # If the request fails, try again after 30 seconds to avoid rate limits\n",
    "        time.sleep(30)\n",
    "        chat.send_message(intro_message)\n",
    "\n",
    "    # Send a question for each label group in BIO format\n",
    "    for group in label_groups:\n",
    "        label_1 = label_mapping[group[0]]\n",
    "        label_2 = label_mapping[group[1]]\n",
    "        prompt = (\n",
    "            f\"\"\"Which are the tokens labeled as '{label_1}' and '{label_2}' in the text? \n",
    "            If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "            The output must be in the format: 'B-PER: entity1, entity2, entity3' & 'I-PER: entity4, entity5'.\n",
    "            If only one category is present, then the output should be: 'B-PER: entity1, entity2, entity3' or 'I-PER: entity4, entity5'.\n",
    "            If both categories have no entities, just answer with 'None'.\"\"\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = chat.send_message(prompt)\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            response = chat.send_message(prompt)\n",
    "\n",
    "        try:\n",
    "            results.update(parse_model_response(response.text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            continue\n",
    "     \n",
    "    # Save result to CSV with ad-hoc functin for Basic Decomposed-QA\n",
    "    save_to_csv_qa(results, true_labels, tokens, \"data/modified_qa_test_light_25.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google import genai\n",
    "from google import generativeai as genai\n",
    "import time\n",
    "\n",
    "for sentence in range(len(sampled_test_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[sentence]['tokens']\n",
    "    true_labels = sampled_test_data[sentence]['ner_tags']\n",
    "    #model = \"gemma-3-27b-it\"\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    model = genai.GenerativeModel(model_name=\"gemma-3-27b-it\")\n",
    "    # Send request \n",
    "    time.sleep(3)\n",
    "    response = model.generate_content(\n",
    "        contents=f\"\"\" Task: Perform Named Entity Recognition (NER) tagging on the sentence below using the BIO tagging scheme.\n",
    "        Sentence: {tokens}\n",
    "        Context: You are a world-class linguist and NER expert. I am the CEO of the most influential NER research company, and I’m asking for your highest-quality tagging for this sentence. Consider each token carefully, and use deep contextual understanding. Think through token sequences internally—such as how BIO tags depend on previous tokens—but do not show your reasoning in the output.\n",
    "        Instructions:\n",
    "        Use the following tag-to-ID mapping:\n",
    "        {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "        Output the corresponding tag ID for each of the {len(tokens)} tokens, in the order they appear.\n",
    "        Format strictly as: ner_tags: 0, 1, 2, 0, 0, 0 (a comma-separated list of integers).\n",
    "        The output must contain exactly {len(tokens)} tag IDs — one for each token.\n",
    "        Do not include any additional commentary, explanation, or formatting beyond the required output.\n",
    "        Note: Apply internal chain-of-thought reasoning as needed to preserve the logic of the BIO tagging format, particularly for entity continuation (I-) tags. However, do not output any intermediate steps.\"\"\",\n",
    "        generation_config={\n",
    "        \"temperature\": 0.2,\n",
    "        # Optionally: \"top_k\": ..., \"top_p\": ...\n",
    "    })\n",
    "    \n",
    "    # Parse the response\n",
    "    data = parse_response(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data/improved_prompt_method_gemma_temp_0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'.\n",
    "            Consider also this POS tags: {{'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    "                'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    "                'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    "                'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    "                'WP': 44, 'WP$': 45, 'WRB': 46}}.\n",
    "            The POS tags for the corresponding tokens are: {sampled_test_data[j]['pos_tags']} \n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'.\n",
    "            Consider also this POS tags: {{'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    "                'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    "                'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    "                'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    "                'WP': 44, 'WP$': 45, 'WRB': 46}}.\n",
    "            The POS tags for the corresponding tokens are: {sampled_test_data[j]['pos_tags']} \n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data/pos_tags_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NO BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "begin_tags = {1,3,5,7}\n",
    "\n",
    "def parse_response_no_bio(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        if i > 0 and temp[i-1][1] in begin_tags and int(response_labels[i].strip()) in begin_tags:\n",
    "            # If the previous token is a begin tag and this is not '0', we assume it's a continuation\n",
    "            pred_label = int(response_labels[i].strip()) + 1\n",
    "        elif i > 0 and int(response_labels[i].strip()) in begin_tags and (temp[i-1][1] - 1) == int(response_labels[i].strip()):\n",
    "            # if previous token is an inside token and the current is in the same category, we assume it's a continuation\n",
    "            pred_label = int(response_labels[i].strip()) + 1\n",
    "        else:\n",
    "            # Otherwise, we take the label as is\n",
    "            pred_label = int(response_labels[i].strip())\n",
    "        if(pred_label < 0 or pred_label > 8):\n",
    "            print(f\"Token: {tokens[i]}, Predicted Label: {pred_label}, True Label: {true_labels[i]}\")\n",
    "        #assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "        #     contents=f\"\"\"Given the following NER tags: {{'O': 0, 'PER': 1, 'ORG': 3, 'LOC': 5, 'MISC': 7}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        #     The sentence is: '{tokens}'\n",
    "        #     This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "        #     Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "        #     Your answer MUST follow the format: ner_tags: 0, 1, 0, 4, 0, 0\n",
    "        #     The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "        #     Do not include any other text or explanations.\n",
    "        #     \"\"\"\n",
    "        # )\n",
    "            contents=f\"\"\"Given entity label set: {{'O': 0, 'PERSON': 1, 'ORGANIZATION': 3, 'LOCATION': 5, 'MISCELLANEOUS': 7}}.\n",
    "            Based on the given entity label set, please recognize the named entities in the given text.\n",
    "            Text: '{tokens}'.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 0, 4, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given entity label set: {{'O': 0, 'PERSON': 1, 'ORGANIZATION': 3, 'LOCATION': 5, 'MISCELLANEOUS': 7}}.\n",
    "            Based on the given entity label set, please recognize the named entities in the given text.\n",
    "            Text: '{tokens}'.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 0, 4, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data/no_bio_test_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
