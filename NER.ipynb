{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/ll/miniconda3/envs/insetti/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ll/miniconda3/envs/insetti/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/ll/miniconda3/envs/insetti/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/ll/miniconda3/envs/insetti/lib/python3.12/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ll/miniconda3/envs/insetti/lib/python3.12/site-packages (from pydantic) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Print a sample\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from the training data:\n",
      "{'id': '11001', 'tokens': ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.'], 'pos_tags': [11, 8, 15, 22, 22, 15, 22, 6, 22, 6, 22, 21, 10, 21, 15, 12, 16, 16, 22, 22, 21, 30, 38, 35, 22, 22, 22, 7], 'chunk_tags': [11, 12, 13, 11, 12, 13, 11, 0, 11, 0, 11, 12, 0, 11, 13, 11, 12, 12, 12, 12, 12, 3, 21, 13, 11, 12, 12, 0], 'ner_tags': [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]}\n",
      "Tokens: ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.']\n",
      "Labels: [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14041\n",
      "Validation set size: 3250\n",
      "Test set size: 3453\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "'''\n",
    "Store the response in a list of lists where the first element is the token, the second element \n",
    "is the predicted label and the third is the true label\n",
    "'''\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    assert (len(response_labels) == len(tokens)), \"Length of tokens and NER tags do not match\"\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        \n",
    "        assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens))]\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pydantic import BaseModel\n",
    "from google import genai\n",
    "\n",
    "for j in range(5):\n",
    "    tokens = train_data[j]['tokens']\n",
    "    true_labels = train_data[j]['ner_tags']\n",
    "    votes = [[] for _ in range(len(tokens))]\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    for i in range(5):\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        # Send request to Gemma\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'\n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        data = parse_response(tokens, response.text, true_labels)\n",
    "        #Â Store predicted labels for voting\n",
    "        store_predicted_labels([data[i][1] for i in range(len(data))], votes)\n",
    "    # Extract for each token the most voted label\n",
    "    votes = [max(set(vote), key=vote.count) for vote in votes]\n",
    "    save_to_csv(tokens, votes, true_labels, \"data/data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def save_basic_decomposed_qa_to_csv(tokens: list, pred_labels: dict, true_labels: dict, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the token, predicted labels, and true labels to a CSV file for Basic Decomposed-QA.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of tokens in the sentence.\n",
    "        pred_labels (dict): Dictionary of predicted labels in the format {'PER': 'entity1, entity2', ...}.\n",
    "        true_labels (dict): Dictionary of true labels in the format {'PER': 'entity1, entity2', ...}.\n",
    "        filename (str): Path to the CSV file (relative to the 'data' folder).\n",
    "    \"\"\"\n",
    "    # Ensure the 'data' directory exists\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Full path to the file in the 'data' directory\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "    \n",
    "    file_exists = os.path.isfile(filepath)\n",
    "    \n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filepath, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    \n",
    "    # Prepare data for each token\n",
    "    data = []\n",
    "    for token in tokens:\n",
    "        predicted_label = None\n",
    "        true_label = None\n",
    "        \n",
    "        # Find the predicted label for the token\n",
    "        for label, entities in pred_labels.items():\n",
    "            if token in entities.split(', '):\n",
    "                predicted_label = label\n",
    "                break\n",
    "        \n",
    "        # Find the true label for the token\n",
    "        for label, entities in true_labels.items():\n",
    "            if token in entities.split(', '):\n",
    "                true_label = label\n",
    "                break\n",
    "        \n",
    "        # Append the token, predicted label, and true label\n",
    "        data.append([token, predicted_label if predicted_label else 'O', true_label if true_label else 'O'])\n",
    "    \n",
    "    # Write data to the CSV file\n",
    "    with open(filepath, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of label numbers to readable tags\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# List of NER labels and their descriptions (without distinction between B- and I-)\n",
    "ner_labels = {\n",
    "    'PER': 'Person',\n",
    "    'ORG': 'Organization',\n",
    "    'LOC': 'Location',\n",
    "    'MISC': 'Miscellaneous'\n",
    "}\n",
    "\n",
    "# Iterate over the first 5 examples of the dataset\n",
    "for j in range(5):\n",
    "    tokens = train_data[j]['tokens']\n",
    "    true_labels = train_data[j]['ner_tags']\n",
    "    votes = [[] for _ in range(len(tokens))]  # Initialize votes for majority voting\n",
    "\n",
    "    # Repeat the process 5 times for majority voting\n",
    "    for iteration in range(5):\n",
    "        # Re-initialize the client for each sentence\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "        # Build a dictionary for the true labels\n",
    "        true_label_dict = {label: [] for label in ner_labels.keys()}\n",
    "        for token, true_label in zip(tokens, true_labels):\n",
    "            readable_label = label_mapping[true_label]\n",
    "            if readable_label != 'O':  # Ignore tokens without labels\n",
    "                main_label = readable_label.split('-')[-1]  # Get PER, ORG, LOC, MISC\n",
    "                true_label_dict[main_label].append(token)\n",
    "\n",
    "        # Convert lists to comma-separated strings\n",
    "        for key in true_label_dict:\n",
    "            true_label_dict[key] = ', '.join(true_label_dict[key]) if true_label_dict[key] else 'None'\n",
    "\n",
    "        context = f\"The sentence is: '{' '.join(tokens)}'\\n\"  # Initial context\n",
    "        results = []  # To save the model's responses\n",
    "\n",
    "        # Iterate over each NER label\n",
    "        for label, description in ner_labels.items():\n",
    "            # Build the question for the current label\n",
    "            question = f\"Question: What are the named entities labeled as '{description}' in the text? You only have to answer with the entity name, if there are multiple entities then separate them with a comma. If there are no entities, answer with 'None'. Finally, remember that a token can only be classified once.\\n\"\n",
    "\n",
    "            # Send the request to the model\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=f\"{context}\\n{question}\\n\"\n",
    "            )\n",
    "\n",
    "            # Save the response and update the context\n",
    "            answer = response.text.strip()\n",
    "            results.append({label: answer})\n",
    "            context += f\"\\n{question}\\n{answer}\\n\"\n",
    "\n",
    "        # Parse the response and update votes\n",
    "        for token_idx, token in enumerate(tokens):\n",
    "            predicted_labels = []\n",
    "            for result in results:\n",
    "                label = list(result.keys())[0]  # Get the label (e.g., 'ORG')\n",
    "                predicted_entities = result[label].split(', ') if result[label] != 'None' else []\n",
    "                if token in predicted_entities:\n",
    "                    predicted_labels.append(label)\n",
    "            # Add the predicted label for this token to the votes\n",
    "            votes[token_idx].append(predicted_labels[0] if predicted_labels else 'O')\n",
    "\n",
    "    # Perform majority voting for each token\n",
    "    final_labels = [max(set(vote), key=vote.count) for vote in votes]\n",
    "\n",
    "    # Build the final predicted labels in the desired format\n",
    "    final_label_dict = {label: [] for label in ner_labels.keys()}\n",
    "    for token, final_label in zip(tokens, final_labels):\n",
    "        if final_label != 'O':  # Ignore tokens without labels\n",
    "            final_label_dict[final_label].append(token)\n",
    "\n",
    "    # Convert lists to comma-separated strings\n",
    "    for key in final_label_dict:\n",
    "        final_label_dict[key] = ', '.join(final_label_dict[key]) if final_label_dict[key] else 'None'\n",
    "\n",
    "    # Print the results for the current sample\n",
    "    print(f\"Results for sample {j + 1}:\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"Predicted Labels:\")\n",
    "    print(final_label_dict)\n",
    "    print(\"True Labels:\")\n",
    "    print(true_label_dict)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    save_basic_decomposed_qa_to_csv(tokens, final_label_dict, true_label_dict, \"basic_decomposed_qa_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insetti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
