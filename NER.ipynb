{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (2.11.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (2.33.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: seqeval in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.40.0)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.11.1)\n",
      "Requirement already satisfied: tqdm in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uritemplate, protobuf, httplib2, grpcio, proto-plus, googleapis-common-protos, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install pydantic\n",
    "!pip install seqeval\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Print a sample\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from the training data:\n",
      "{'id': '11001', 'tokens': ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.'], 'pos_tags': [11, 8, 15, 22, 22, 15, 22, 6, 22, 6, 22, 21, 10, 21, 15, 12, 16, 16, 22, 22, 21, 30, 38, 35, 22, 22, 22, 7], 'chunk_tags': [11, 12, 13, 11, 12, 13, 11, 0, 11, 0, 11, 12, 0, 11, 13, 11, 12, 12, 12, 12, 12, 3, 21, 13, 11, 12, 12, 0], 'ner_tags': [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]}\n",
      "Tokens: ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.']\n",
      "Labels: [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14041\n",
      "Validation set size: 3250\n",
      "Test set size: 3453\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        \n",
    "        assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv_vanilla(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens)) if pred_labels[i] != 0 and true_labels[i] != 0]\n",
    "    # Remove duplicates\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "{'id': '1645', 'tokens': ['12/15/2007', '850M', '4.85', '4.85'], 'pos_tags': [11, 11, 11, 11], 'chunk_tags': [11, 12, 12, 12], 'ner_tags': [0, 0, 0, 0]}\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "{'id': '986', 'tokens': ['Tambang', 'Timah', 'at', '$', '15.575', 'in', 'London', '.'], 'pos_tags': [39, 22, 15, 3, 11, 15, 22, 7], 'chunk_tags': [21, 11, 13, 11, 12, 13, 11, 0], 'ner_tags': [3, 4, 0, 0, 0, 0, 5, 0]}\n",
      "\n",
      "\n",
      "Sample 3:\n",
      "{'id': '529', 'tokens': ['SAN', 'FRANCISCO', '10', '3', '0', '325', '198'], 'pos_tags': [22, 22, 11, 11, 11, 11, 11], 'chunk_tags': [11, 12, 12, 12, 12, 12, 12], 'ner_tags': [3, 4, 0, 0, 0, 0, 0]}\n",
      "\n",
      "\n",
      "Sample 4:\n",
      "{'id': '302', 'tokens': ['The', 'West', 'Indies', 'vice-captain', 'struggled', 'for', 'timing', 'during', 'his', '36-minute', 'stay', 'at', 'the', 'crease', 'before', 'chipping', 'a', 'ball', 'from', 'medium', 'pacer', 'Tom', 'Moody', 'straight', 'to', 'Shane', 'Warne', 'at', 'mid-wicket', '.'], 'pos_tags': [12, 22, 22, 16, 38, 15, 21, 15, 29, 16, 21, 15, 12, 21, 15, 39, 12, 21, 15, 21, 21, 22, 22, 30, 35, 22, 22, 15, 22, 7], 'chunk_tags': [11, 12, 12, 12, 21, 13, 11, 13, 11, 12, 12, 13, 11, 12, 13, 21, 11, 12, 13, 11, 12, 12, 12, 3, 13, 11, 12, 13, 11, 0], 'ner_tags': [0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 0]}\n",
      "\n",
      "\n",
      "Sample 5:\n",
      "{'id': '3350', 'tokens': ['W', 'L', 'T', 'GF', 'GA', 'PTS'], 'pos_tags': [22, 22, 22, 22, 22, 22], 'chunk_tags': [11, 12, 12, 12, 12, 12], 'ner_tags': [0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample 300 random elements from the test set\n",
    "sampled_test_data = random.sample(list(test_data), 300)\n",
    "\n",
    "# Print the first few samples to verify\n",
    "for i, sample in enumerate(sampled_test_data[:5]):  # Display the first 5 samples\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(sample)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    # Send request \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        The sentence is: '{tokens}'\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "        Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "        Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "        The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "        Do not include any other text or explanations.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data/vanilla_test_300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import os\n",
    "\n",
    "# label_mapping = {\n",
    "#     0: 'O',\n",
    "#     1: 'B-PER',\n",
    "#     2: 'I-PER',\n",
    "#     3: 'B-ORG',\n",
    "#     4: 'I-ORG',\n",
    "#     5: 'B-LOC',\n",
    "#     6: 'I-LOC',\n",
    "#     7: 'B-MISC',\n",
    "#     8: 'I-MISC'\n",
    "# }\n",
    "\n",
    "# def save_basic_decomposed_qa_to_csv(tokens: list, pred_labels: dict, true_labels: dict, filename: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Save the token, predicted labels, and true labels to a CSV file for Basic Decomposed-QA.\n",
    "\n",
    "#     Args:\n",
    "#         tokens (list): List of tokens in the sentence.\n",
    "#         pred_labels (dict): Dictionary of predicted labels in the format {'PER': ['entity1', 'entity2'], ...}.\n",
    "#         true_labels (dict): Dictionary of true labels in the format {'PER': ['entity1', 'entity2'], ...}.\n",
    "#         filename (str): Path to the CSV file (relative to the 'data' folder).\n",
    "#     \"\"\"\n",
    "#     # Ensure the 'data' directory exists\n",
    "#     os.makedirs(\"data\", exist_ok=True)\n",
    "#     filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "#     file_exists = os.path.isfile(filepath)\n",
    "\n",
    "#     if not file_exists:\n",
    "#         with open(filepath, 'w', newline='') as csvfile:\n",
    "#             writer = csv.writer(csvfile)\n",
    "#             writer.writerow(['token', 'pred', 'true'])\n",
    "\n",
    "#     # Create reverse lookup from token to label index\n",
    "#     def get_label_index(token_idx, label_dict):\n",
    "#         for label_str, entities in label_dict.items():\n",
    "#             if entities == 'None':\n",
    "#                 continue\n",
    "#             tokens_in_label = [t.strip() for t in entities.split(',')]\n",
    "#             if tokens[token_idx] in tokens_in_label:\n",
    "#                 # Convert label to index\n",
    "#                 for idx, tag in label_mapping.items():\n",
    "#                     if tag == label_str:\n",
    "#                         return idx\n",
    "#         return 0  # 'O'\n",
    "\n",
    "#     # Write data\n",
    "#     with open(filepath, 'a', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         for i, token in enumerate(tokens):\n",
    "#             pred_idx = get_label_index(i, pred_labels)\n",
    "#             true_idx = get_label_index(i, true_labels)\n",
    "#             writer.writerow([token, pred_idx, true_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5513626834381551\n",
      "Recall: 0.6368038740920097\n",
      "F1 Score: 0.5910112359550561\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.56      0.86      0.68       139\n",
      "        MISC       0.43      0.29      0.35        41\n",
      "         ORG       0.26      0.23      0.24       118\n",
      "         PER       0.79      0.90      0.84       115\n",
      "\n",
      "   micro avg       0.55      0.64      0.59       413\n",
      "   macro avg       0.51      0.57      0.53       413\n",
      "weighted avg       0.53      0.64      0.57       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "category_to_index = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, \n",
    "    'I-PER': 2, \n",
    "    'B-ORG': 3, \n",
    "    'I-ORG': 4, \n",
    "    'B-LOC': 5, \n",
    "    'I-LOC': 6, \n",
    "    'B-MISC': 7, \n",
    "    'I-MISC': 8\n",
    "    }\n",
    "\n",
    "# Step 2: Read the CSV and convert predictions and true labels\n",
    "true_seqs = []\n",
    "pred_seqs = []\n",
    "current_true = []\n",
    "current_pred = []\n",
    "\n",
    "with open('data/vanilla_test_300.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        token = row['token']\n",
    "        # true_label = index_to_label[int(row['true'])]\n",
    "        # pred_label = index_to_label[int(row['pred'])]\n",
    "        true_label = label_mapping[int(row['true'])]\n",
    "        pred_label = label_mapping[int(row['pred'])]\n",
    "\n",
    "        current_true.append(true_label)\n",
    "        current_pred.append(pred_label)\n",
    "\n",
    "    true_seqs.append(current_true)\n",
    "    pred_seqs.append(current_pred)\n",
    "\n",
    "# Step 3: Compute metrics\n",
    "print(\"Precision:\", precision_score(true_seqs, pred_seqs))\n",
    "print(\"Recall:\", recall_score(true_seqs, pred_seqs))\n",
    "print(\"F1 Score:\", f1_score(true_seqs, pred_seqs))\n",
    "\n",
    "# Optional detailed report\n",
    "print(\"\\nDetailed classification report:\\n\")\n",
    "print(classification_report(true_seqs, pred_seqs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: ['B-PER: Marshall, Jason', 'I-PER: Faulk, Belser']\n",
      "Answer: B-PER: Marshall, Jason\n",
      "Answer: I-PER: Faulk, Belser\n",
      "Answers: ['B-ORG: Colts, Eagles', 'I-ORG:']\n",
      "Answer: B-ORG: Colts, Eagles\n",
      "Answer: I-ORG:\n",
      "True label dictionary: {'Marshall': 'B-PER', 'Faulk': 'I-PER', 'rushed': 'O', 'for': 'O', '101': 'O', 'yards': 'O', 'and': 'O', 'two': 'O', 'touchdowns': 'O', 'Jason': 'B-PER', 'Belser': 'I-PER', 'returned': 'O', 'an': 'O', 'interception': 'O', '44': 'O', 'a': 'O', 'score': 'O', 'as': 'O', 'the': 'O', 'Colts': 'B-ORG', 'improved': 'O', 'to': 'O', '8-6': 'O', ',': 'O', 'same': 'O', 'mark': 'O', 'Eagles': 'B-ORG', 'who': 'O', 'lost': 'O', 'fourth': 'O', 'time': 'O', 'in': 'O', 'five': 'O', 'games': 'O', '.': 'O'}\n",
      "Predicted labels: {'B-PER': 'Marshall, Jason', 'I-PER': 'Faulk, Belser', 'B-ORG': 'Colts, Eagles'}\n",
      "Data written to data/qa_test_light.csv\n",
      "Answers: ['B-PER: Kwasniewski, Oscar', 'I-PER: Scalfaro']\n",
      "Answer: B-PER: Kwasniewski, Oscar\n",
      "Answer: I-PER: Scalfaro\n",
      "Answers: ['B-LOC: Italy']\n",
      "Answer: B-LOC: Italy\n",
      "True label dictionary: {'\"': 'O', 'President': 'O', 'Kwasniewski': 'B-PER', 'plans': 'O', 'to': 'O', 'visit': 'O', 'Italy': 'B-LOC', 'on': 'O', 'a': 'O', 'invitation': 'O', 'from': 'O', 'Oscar': 'B-PER', 'Scalfaro': 'I-PER', '.': 'O'}\n",
      "Predicted labels: {'B-PER': 'Kwasniewski, Oscar', 'I-PER': 'Scalfaro', 'B-LOC': 'Italy'}\n",
      "Data written to data/qa_test_light.csv\n",
      "True label dictionary: {'Note': 'O', '-': 'O', 'all': 'O', 'cattle': 'O', 'prices': 'O', 'based': 'O', 'on': 'O', 'net': 'O', 'weights': 'O', 'FOB': 'O', 'the': 'O', 'feedlot': 'O', 'after': 'O', 'a': 'O', '4': 'O', 'percent': 'O', 'shrink': 'O', '.': 'O'}\n",
      "Predicted labels: {}\n",
      "Data written to data/qa_test_light.csv\n",
      "True label dictionary: {'Winds': 'O', 'from': 'O', 'the': 'O', 'northeast': 'O', 'of': 'O', '10': 'O', 'to': 'O', '15': 'O', 'knots': 'O', '(': 'O', '19': 'O', '28': 'O', 'kilometers': 'O', '/': 'O', '11': 'O', '17': 'O', 'miles': 'O', 'per': 'O', 'hour': 'O', ')': 'O', '.': 'O'}\n",
      "Predicted labels: {}\n",
      "Data written to data/qa_test_light.csv\n",
      "Answers: ['B-MISC: MAHINDRA', 'I-MISC: INTERNATIONAL']\n",
      "Answer: B-MISC: MAHINDRA\n",
      "Answer: I-MISC: INTERNATIONAL\n",
      "True label dictionary: {'SQUASH': 'O', '-': 'O', 'MAHINDRA': 'B-MISC', 'INTERNATIONAL': 'I-MISC', 'SEMIFINAL': 'O', 'RESULTS': 'O', '.': 'O'}\n",
      "Predicted labels: {'B-MISC': 'MAHINDRA', 'I-MISC': 'INTERNATIONAL'}\n",
      "Data written to data/qa_test_light.csv\n",
      "Answers: ['B-ORG: Bharatiya, BJP', 'I-ORG: Janata, Party']\n",
      "Answer: B-ORG: Bharatiya, BJP\n",
      "Answer: I-ORG: Janata, Party\n",
      "Answers: ['B-MISC: Babri', 'I-MISC: mosque']\n",
      "Answer: B-MISC: Babri\n",
      "Answer: I-MISC: mosque\n",
      "True label dictionary: {'Members': 'O', 'of': 'O', 'the': 'O', 'Hindu': 'B-MISC', 'nationalist': 'O', 'Bharatiya': 'B-ORG', 'Janata': 'I-ORG', 'Party': 'I-ORG', '(': 'O', 'BJP': 'B-ORG', ')': 'O', 'shouted': 'O', 'pro-Hindu': 'B-MISC', 'slogans': 'O', 'in': 'O', 'house': 'O', 'after': 'O', 'a': 'O', 'communist': 'O', 'deputy': 'O', 'made': 'O', 'proposal': 'O', 'remembrance': 'O', 'Babri': 'O', 'mosque': 'O', ',': 'O', 'which': 'O', 'was': 'O', 'razed': 'O', 'on': 'O', 'December': 'O', '6': 'O', '1992': 'O', '.': 'O'}\n",
      "Predicted labels: {'B-ORG': 'Bharatiya, BJP', 'I-ORG': 'Janata, Party', 'B-MISC': 'Babri', 'I-MISC': 'mosque'}\n",
      "Data written to data/qa_test_light.csv\n",
      "Answers: ['B-PER: Lara']\n",
      "Answer: B-PER: Lara\n",
      "Answers: ['B-LOC: West, Australia', 'I-LOC: Indies']\n",
      "Answer: B-LOC: West, Australia\n",
      "Answer: I-LOC: Indies\n",
      "True label dictionary: {'Lara': 'B-PER', 'has': 'O', 'yet': 'O', 'to': 'O', 'score': 'O', 'a': 'O', 'century': 'O', 'since': 'O', 'West': 'B-LOC', 'Indies': 'I-LOC', 'arrived': 'O', 'in': 'O', 'Australia': 'B-LOC', 'five': 'O', 'weeks': 'O', 'ago': 'O', '.': 'O'}\n",
      "Predicted labels: {'B-PER': 'Lara', 'B-LOC': 'West, Australia', 'I-LOC': 'Indies'}\n",
      "Data written to data/qa_test_light.csv\n",
      "Answers: ['B-LOC: New', 'I-LOC: York']\n",
      "Answer: B-LOC: New\n",
      "Answer: I-LOC: York\n",
      "Answers: ['B-MISC: Mexican']\n",
      "Answer: B-MISC: Mexican\n",
      "True label dictionary: {'Traders': 'O', 'also': 'O', 'remarked': 'O', 'that': 'O', 'Mexican': 'B-MISC', 'ADRs': 'B-MISC', 'suffered': 'O', 'in': 'O', 'New': 'B-LOC', 'York': 'I-LOC', '.': 'O'}\n",
      "Predicted labels: {'B-LOC': 'New', 'I-LOC': 'York', 'B-MISC': 'Mexican'}\n",
      "Data written to data/qa_test_light.csv\n",
      "Answers: ['B-PER: David', 'I-PER: Campese']\n",
      "Answer: B-PER: David\n",
      "Answer: I-PER: Campese\n",
      "Answers: ['B-LOC: England']\n",
      "Answer: B-LOC: England\n",
      "True label dictionary: {'\"': 'O', 'I': 'O', \"'m\": 'O', 'sure': 'O', 'there': 'O', 'are': 'O', 'a': 'O', 'few': 'O', 'people': 'O', 'in': 'O', 'England': 'B-LOC', 'who': 'O', \"'d\": 'O', 'be': 'O', 'delighted': 'O', 'to': 'O', 'have': 'O', 'David': 'B-PER', 'Campese': 'I-PER', 'their': 'O', 'club': 'O', \"'s\": 'O', 'jersey': 'O', ',': 'O', 'he': 'O', 'said': 'O', '.': 'O'}\n",
      "Predicted labels: {'B-PER': 'David', 'I-PER': 'Campese', 'B-LOC': 'England'}\n",
      "Data written to data/qa_test_light.csv\n"
     ]
    }
   ],
   "source": [
    "#Decomposed-QA\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "    \n",
    "def save_to_csv_qa(pred_labels, true_labels_indices, tokens, filename):\n",
    "    data = []\n",
    "    \n",
    "    # Convert true_labels_indices to a dictionary mapping tokens to their true labels\n",
    "    true_label_dict = {}\n",
    "    for token, label_idx in zip(tokens, true_labels_indices):\n",
    "        true_label_dict[token] = label_mapping[label_idx]\n",
    "    \n",
    "    print(\"True label dictionary:\", true_label_dict)\n",
    "    print(\"Predicted labels:\", pred_labels)\n",
    "    \n",
    "    # Process each predicted label and their tokens\n",
    "    for pred_label, tokens_str in pred_labels.items():\n",
    "        # Split and remove whitespace from the predicted tokens\n",
    "        token_list = [t.strip() for t in tokens_str.split(',')]\n",
    "        \n",
    "        # Check if any of these tokens are in our original token list\n",
    "        # If not, they might be composite entities that need special handling\n",
    "        for token in token_list:\n",
    "            if token in tokens:\n",
    "                # Direct match with a single token\n",
    "                true_label = true_label_dict.get(token, 'O')\n",
    "                data.append([token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "            else:\n",
    "                # This might be a composite token or doesn't exist in the original tokens\n",
    "                # Try to match with individual tokens from the original list\n",
    "                token_found = False\n",
    "                for orig_token in tokens:\n",
    "                    # Check if this token is part of the composite or has been recognized separately\n",
    "                    if orig_token == token or token.find(orig_token) >= 0:\n",
    "                        true_label = true_label_dict.get(orig_token, 'O')\n",
    "                        data.append([orig_token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "                        token_found = True\n",
    "                \n",
    "                if not token_found:\n",
    "                    # If we can't match it, add it with the predicted label but mark true as 'O'\n",
    "                    print(f\"Warning: Could not match predicted token '{token}' to original tokens\")\n",
    "                    data.append([token, category_to_index[pred_label], category_to_index['O']])\n",
    "    \n",
    "    # Process any tokens that weren't in the predictions but have true labels\n",
    "    for token, true_label in true_label_dict.items():\n",
    "        if true_label != 'O':\n",
    "            # Check if this token was already processed\n",
    "            token_processed = any(entry[0] == token for entry in data)\n",
    "            if not token_processed:\n",
    "                data.append([token, category_to_index['O'], category_to_index[true_label]])\n",
    "    \n",
    "\n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "label_groups = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')  # or gemma-3-27b-it\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    results = {}\n",
    "    \n",
    "    chat = model.start_chat()\n",
    "    # Send general instructions only at the beginning of the conversation\n",
    "    intro_message = (\n",
    "        f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "        \n",
    "        The sentence is: '{tokens}'.\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "        Remember that a token can only be classified once.\"\"\"\n",
    "    )\n",
    "    # Send message to the chat\n",
    "    try:\n",
    "        chat.send_message(intro_message)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        # If the request fails, try again after 30 seconds to avoid rate limits\n",
    "        time.sleep(30)\n",
    "        chat.send_message(intro_message)\n",
    "\n",
    "    # Send a question for each label group in BIO format\n",
    "    for group in label_groups:\n",
    "        label_1 = label_mapping[group[0]]\n",
    "        label_2 = label_mapping[group[1]]\n",
    "        prompt = (\n",
    "            f\"\"\"Which are the tokens labeled as '{label_1}' and '{label_2}' in the text? \n",
    "            If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "            The output must be in the format: 'B-PER: entity1, entity2, entity3' & 'I-PER: entity4, entity5'.\n",
    "            If only one category is present, then the output should be: 'B-PER: entity1, entity2, entity3' or 'I-PER: entity4, entity5'.\n",
    "            If both categories have no entities, just answer with 'None'.\"\"\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = chat.send_message(prompt)\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            response = chat.send_message(prompt)\n",
    "\n",
    "\n",
    "        if (response.text != 'None'):\n",
    "            answers = response.text.split('&')\n",
    "            answers = [answer.strip() for answer in answers]\n",
    "            print(\"Answers:\", answers)\n",
    "            for answer in answers:\n",
    "                answer = answer.strip()\n",
    "                print(\"Answer:\", answer)\n",
    "                if answer != 'None':\n",
    "                    # Check if the answar contains a colon\n",
    "                    if ':' in answer:\n",
    "                        # Handle the case where there might be nothing after the colon\n",
    "                        # Split on the first colon only\n",
    "                        parts = answer.split(':', 1)  \n",
    "                        label = parts[0].strip()\n",
    "                        \n",
    "                        # Check if there's content after the colon\n",
    "                        entities = parts[1].strip() if len(parts) > 1 and parts[1].strip() else \"\"\n",
    "                        \n",
    "                        # Only update results if we actually have entities\n",
    "                        if entities:\n",
    "                            results.update({label: entities})\n",
    "                        else:\n",
    "                            continue\n",
    "                            #print(f\"Warning: No entities found for label {label}\")\n",
    "                    else:\n",
    "                        continue\n",
    "                        #print(f\"Warning: Invalid format in answer: {answer}\")\n",
    "            \n",
    "    # Save result to CSV with ad-hoc functin for Basic Decomposed-QA\n",
    "    save_to_csv_qa(results, true_labels, tokens, \"data/qa_test_light.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_labels: [0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 0]\n",
      "true_labels: ['The', 'West', 'Indies', 'vice-captain', 'struggled', 'for', 'timing', 'during', 'his', '36-minute', 'stay', 'at', 'the', 'crease', 'before', 'chipping', 'a', 'ball', 'from', 'medium', 'pacer', 'Tom', 'Moody', 'straight', 'to', 'Shane', 'Warne', 'at', 'mid-wicket', '.']\n",
      "[]\n",
      "{'O': [], 'B-PER': ['Tom', 'Shane'], 'I-PER': ['Moody', 'Warne'], 'B-ORG': [], 'I-ORG': [], 'B-LOC': ['West'], 'I-LOC': ['Indies'], 'B-MISC': [], 'I-MISC': []}\n"
     ]
    }
   ],
   "source": [
    "# print('true_labels:', sampled_test_data[3]['ner_tags'])\n",
    "# print('true_labels:', sampled_test_data[3]['tokens'])\n",
    "# data = []\n",
    "# for i, result in enumerate(results):\n",
    "#     for label, tokens in result.items():\n",
    "#         #print(label, tokens)\n",
    "#         tokens = tokens.split(', ')\n",
    "#         for token in tokens:\n",
    "#  #           print(token, label, true_labels[i])\n",
    "#             data.append([token.strip(), label])\n",
    "# print(data)\n",
    "\n",
    "\n",
    "# tokens = sampled_test_data[3]['tokens']\n",
    "# true_labels = sampled_test_data[3]['ner_tags']\n",
    "# true_label_dict = {label: [] for label in label_mapping.values()}\n",
    "# for token, true_label in zip(tokens, true_labels):\n",
    "#     label_str = label_mapping[true_label]\n",
    "#     if label_str != 'O':\n",
    "#         true_label_dict[label_str].append(token)\n",
    "\n",
    "# print(true_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7281962411375407\n",
      "Recall: 0.8122881731878271\n",
      "F1 Score: 0.7430039993271146\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.87      0.80        39\n",
      "        MISC       0.11      0.50      0.18        10\n",
      "           O       0.99      0.92      0.96       578\n",
      "         ORG       0.83      0.77      0.80        39\n",
      "         PER       0.97      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.73      0.81      0.74       726\n",
      "weighted avg       0.96      0.91      0.93       726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "# from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Load true and predicted labels\n",
    "# true_labels = []\n",
    "# pred_labels = []\n",
    "\n",
    "# with open('data/qa_results.csv', newline='') as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for row in reader:\n",
    "#         true_labels.append(row['true'])\n",
    "#         pred_labels.append(row['pred'])\n",
    "\n",
    "# # Compute token-level metrics\n",
    "# print(\"Precision:\", precision_score(true_labels, pred_labels, average='macro', zero_division=0))\n",
    "# print(\"Recall:\", recall_score(true_labels, pred_labels, average='macro', zero_division=0))\n",
    "# print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='macro', zero_division=0))\n",
    "\n",
    "# # Detailed report\n",
    "# print(\"\\nClassification Report:\\n\")\n",
    "# print(classification_report(true_labels, pred_labels, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
