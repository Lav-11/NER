{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-genai\n",
    "%pip install seqeval\n",
    "%pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(0)\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Print a sample\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        if(pred_label < 0 or pred_label > 8):\n",
    "            print(f\"Token: {tokens[i]}, Predicted Label: {pred_label}, True Label: {true_labels[i]}\")\n",
    "        #assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv_vanilla(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    if (len(pred_labels) == 0 and len(true_labels) == 0):\n",
    "        return\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens)) if pred_labels[i] != 0 or true_labels[i] != 0]\n",
    "    # Remove duplicates\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 300 random elements from the test set\n",
    "sample_train_data = random.sample(list(train_data), 300)\n",
    "sampled_test_data = random.sample(list(test_data), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "category_to_index = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, \n",
    "    'I-PER': 2, \n",
    "    'B-ORG': 3, \n",
    "    'I-ORG': 4, \n",
    "    'B-LOC': 5, \n",
    "    'I-LOC': 6, \n",
    "    'B-MISC': 7, \n",
    "    'I-MISC': 8\n",
    "    }\n",
    "\n",
    "valid_labels = set(label_mapping.values())\n",
    "\n",
    "def evaluate_predictions(filename: str) -> None:\n",
    "    true_seqs = []\n",
    "    pred_seqs = []\n",
    "    current_true = []\n",
    "    current_pred = []\n",
    "    invalid_count = 0\n",
    "\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            true_index = int(row['true'])\n",
    "            pred_index = int(row['pred'])\n",
    "            \n",
    "            true_label = label_mapping.get(true_index, 'O')\n",
    "            pred_label = label_mapping.get(pred_index, 'INVALID')\n",
    "\n",
    "            if pred_label not in valid_labels:\n",
    "                pred_label = 'INVALID'\n",
    "                invalid_count += 1\n",
    "                continue\n",
    "\n",
    "            current_true.append(true_label)\n",
    "            current_pred.append(pred_label)\n",
    "\n",
    "        true_seqs.append(current_true)\n",
    "        pred_seqs.append(current_pred)\n",
    "\n",
    "    print(f\"Invalid predictions: {invalid_count}\\n\")\n",
    "\n",
    "    print(\"Precision:\", precision_score(true_seqs, pred_seqs))\n",
    "    print(\"Recall:\", recall_score(true_seqs, pred_seqs))\n",
    "    print(\"F1 Score:\", f1_score(true_seqs, pred_seqs))\n",
    "\n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(classification_report(true_seqs, pred_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_ner_confusion_matrix_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Generates and visualizes a confusion matrix for NER data from a CSV file.\n",
    "\n",
    "    The CSV file is expected to have headers: 'token', 'pred', 'true'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing the data.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure the required columns exist\n",
    "        if 'pred' not in df.columns or 'true' not in df.columns:\n",
    "            print(\"Error: CSV file must contain 'pred' and 'true' columns.\")\n",
    "            return\n",
    "\n",
    "        # Extract predicted and true label indices\n",
    "        predicted_indices = df['pred'].tolist()\n",
    "        true_indices = df['true'].tolist()\n",
    "\n",
    "        # Convert indices to string labels using the mapping\n",
    "        predicted_labels_flat = [label_mapping.get(idx, 'UNKNOWN') for idx in predicted_indices]\n",
    "        true_labels_flat = [label_mapping.get(idx, 'UNKNOWN') for idx in true_indices]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get all unique labels present in the mapping to ensure all classes are in the matrix\n",
    "    all_possible_labels = sorted(list(label_mapping.values()))\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_labels_flat, predicted_labels_flat, labels=all_possible_labels)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=all_possible_labels,\n",
    "                yticklabels=all_possible_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix for NER Tags')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "begin_tags = {1,3,5,7}\n",
    "\n",
    "def parse_response_no_bio(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        if i > 0 and temp[i-1][1] in begin_tags and int(response_labels[i].strip()) in begin_tags:\n",
    "            # If the previous token is a begin tag and this is not '0', we assume it's a continuation\n",
    "            pred_label = int(response_labels[i].strip()) + 1\n",
    "        elif i > 0 and int(response_labels[i].strip()) in begin_tags and (temp[i-1][1] - 1) == int(response_labels[i].strip()):\n",
    "            # if previous token is an inside token and the current is in the same category, we assume it's a continuation\n",
    "            pred_label = int(response_labels[i].strip()) + 1\n",
    "        else:\n",
    "            # Otherwise, we take the label as is\n",
    "            pred_label = int(response_labels[i].strip())\n",
    "        if(pred_label < 0 or pred_label > 8):\n",
    "            print(f\"Token: {tokens[i]}, Predicted Label: {pred_label}, True Label: {true_labels[i]}\")\n",
    "        #assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sample_train_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'\n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "        time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/vanilla_test_bio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/vanilla_test_bio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposed-QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposed-QA\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "\n",
    "def save_to_csv_qa(pred_labels, true_labels_indices, tokens, filename):\n",
    "    data = []\n",
    "    \n",
    "    # Convert true_labels_indices to a dictionary mapping tokens to their true labels\n",
    "    true_label_dict = {}\n",
    "    for token, label_idx in zip(tokens, true_labels_indices):\n",
    "        # Ensure we're using a valid label from label_mapping\n",
    "        if label_idx in label_mapping:\n",
    "            true_label_dict[token] = label_mapping[label_idx]\n",
    "        else:\n",
    "            print(f\"Warning: Unknown label index {label_idx} for token '{token}', defaulting to 'O'\")\n",
    "            true_label_dict[token] = 'O'\n",
    "    \n",
    "    # Process each predicted label and their tokens\n",
    "    for pred_label, tokens_str in pred_labels.items():\n",
    "        # Skip invalid labels\n",
    "        if pred_label == 'None' or pred_label not in category_to_index:\n",
    "            print(f\"Warning: Skipping invalid label '{pred_label}'\")\n",
    "            continue\n",
    "            \n",
    "        # Split by commas to get individual tokens\n",
    "        token_list = [t.strip() for t in tokens_str.split(',')]\n",
    "        \n",
    "        for token in token_list:\n",
    "            if not token:  # Skip empty tokens\n",
    "                continue\n",
    "                \n",
    "            # Check if token exists in original tokens\n",
    "            if token in tokens:\n",
    "                # Direct match with a single token\n",
    "                true_label = true_label_dict.get(token, 'O')\n",
    "                data.append([token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "            else:\n",
    "                # This could be a composite token or doesn't exist in our original tokens\n",
    "                # Try to match with individual tokens from the original list\n",
    "                matched_tokens = []\n",
    "                for orig_token in tokens:\n",
    "                    # Find approximate matches\n",
    "                    #if orig_token.lower() in token.lower() or token.lower() in orig_token.lower():\n",
    "                    if token.strip().lower() == orig_token.strip().lower():\n",
    "                        true_label = true_label_dict.get(orig_token, 'O')\n",
    "                        data.append([orig_token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "                        matched_tokens.append(orig_token)\n",
    "                \n",
    "                # If we still couldn't match it to any original tokens\n",
    "                if not matched_tokens:\n",
    "                    # Only add unknown tokens if they look reasonable (not empty, not \"None\", etc.)\n",
    "                    if token != \"None\" and len(token) > 1:\n",
    "                        #print(f\"Warning: Could not match predicted token '{token}' to original tokens\")\n",
    "                        # We'll add it with 'O' as the true label since we can't find it\n",
    "                        data.append([token, category_to_index[pred_label], category_to_index['O']])\n",
    "    \n",
    "    # Process any tokens that weren't in the predictions but have true labels\n",
    "    for token, true_label in true_label_dict.items():\n",
    "        if true_label != 'O':  # Only include non-O labels\n",
    "            # Check if this token was already processed\n",
    "            token_processed = any(entry[0] == token for entry in data)\n",
    "            if not token_processed:\n",
    "                data.append([token, category_to_index['O'], category_to_index[true_label]])\n",
    "    \n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    #print(f\"Data written to {filename}\")\n",
    "\n",
    "# Function to parse model responses and extract predicted labels\n",
    "def parse_model_response(response_text):\n",
    "    results = {}\n",
    "    \n",
    "    if response_text == 'None':\n",
    "        return results\n",
    "        \n",
    "    # Split by & to get multiple label groups\n",
    "    answers = response_text.split('&')\n",
    "    answers = [answer.strip() for answer in answers]\n",
    "    \n",
    "    for answer in answers:\n",
    "        answer = answer.strip()\n",
    "        \n",
    "        # Skip empty answers or explicit \"None\" answers\n",
    "        if not answer or answer == 'None':\n",
    "            continue\n",
    "            \n",
    "        # Check if we have a proper format with colon\n",
    "        if ':' in answer:\n",
    "            parts = answer.split(':', 1)  # Split on first colon only\n",
    "            label = parts[0].strip().strip(\"'\")\n",
    "            \n",
    "            # Validate the label before adding\n",
    "            if label not in category_to_index:\n",
    "                print(f\"Warning: Skipping invalid label format: '{label}'\")\n",
    "                continue\n",
    "                \n",
    "            # Extract entities (everything after the colon)\n",
    "            entities = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            for known_label in category_to_index.keys():\n",
    "                if known_label + ':' in entities:\n",
    "                    entities = entities.replace(known_label + ':', '')\n",
    "\n",
    "            \n",
    "            # Only add if we have non-empty entities\n",
    "            if entities:\n",
    "                results[label] = entities\n",
    "        else:\n",
    "            print(f\"Warning: Invalid format in answer: '{answer}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "label_groups = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "#model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')  # or gemma-3-27b-it\n",
    "#model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "model = genai.GenerativeModel('gemma-3-27b-it') \n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    results = {}\n",
    "    print(\"Tokens:\", tokens)\n",
    "    chat = model.start_chat()\n",
    "    # Send general instructions only at the beginning of the conversation\n",
    "    intro_message = (\n",
    "        f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "        \n",
    "        The sentence is: '{tokens}'.\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "        Remember that a token can only be classified once.\"\"\"\n",
    "    )\n",
    "    # Send message to the chat\n",
    "    try:\n",
    "        chat.send_message(intro_message)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        # If the request fails, try again after 30 seconds to avoid rate limits\n",
    "        time.sleep(30)\n",
    "        chat.send_message(intro_message)\n",
    "\n",
    "    # Send a question for each label group in BIO format\n",
    "    for group in label_groups:\n",
    "        label_1 = label_mapping[group[0]]\n",
    "        label_2 = label_mapping[group[1]]\n",
    "        prompt = (\n",
    "            f\"\"\"Which are the tokens labeled as '{label_1}' and '{label_2}' in the text? \n",
    "            If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "            The output must be in the format: B-PER: entity1, entity2, entity3 & I-PER: entity4, entity5.\n",
    "            If only one category is present, then the output should be: B-PER: entity1, entity2, entity3 or I-PER: entity4, entity5.\n",
    "            If both categories have no entities, just answer with 'None'.\"\"\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = chat.send_message(prompt)\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            response = chat.send_message(prompt)\n",
    "\n",
    "        try:\n",
    "            results.update(parse_model_response(response.text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            continue\n",
    "    print(\"Response text:\", results)\n",
    "    # Save result to CSV with ad-hoc functin for Basic Decomposed-QA\n",
    "    save_to_csv_qa(results, true_labels, tokens, \"data_train_gemma/decomposed_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/decomposed_qa.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google import genai\n",
    "from google import generativeai as genai\n",
    "import time\n",
    "\n",
    "for sentence in range(len(sample_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[sentence]['tokens']\n",
    "    true_labels = sample_train_data[sentence]['ner_tags']\n",
    "    #model = \"gemma-3-27b-it\"\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    model = genai.GenerativeModel(model_name=\"gemma-3-27b-it\")\n",
    "    # Send request \n",
    "    time.sleep(3)\n",
    "    response = model.generate_content(\n",
    "        contents=f\"\"\" Task: Perform Named Entity Recognition (NER) tagging on the sentence below using the BIO tagging scheme.\n",
    "        Sentence: {tokens}\n",
    "        Context: You are a world-class linguist and NER expert. I am the CEO of the most influential NER research company, and I’m asking for your highest-quality tagging for this sentence. Consider each token carefully, and use deep contextual understanding. Think through token sequences internally—such as how BIO tags depend on previous tokens—but do not show your reasoning in the output.\n",
    "        Instructions:\n",
    "        Use the following tag-to-ID mapping:\n",
    "        {{'O': 0, 'PERSON': 1, 'ORGANIZATION': 3, 'LOCATION': 5, 'MISCELLANEOUS': 7}}\n",
    "        Output the corresponding tag ID for each of the {len(tokens)} tokens, in the order they appear.\n",
    "        Format strictly as: ner_tags: 0, 1, 2, 0, 0, 0 (a comma-separated list of integers).\n",
    "        The output must contain exactly {len(tokens)} tag IDs — one for each token.\n",
    "        Do not include any additional commentary, explanation, or formatting beyond the required output.\n",
    "        Note: Apply internal chain-of-thought reasoning as needed to preserve the logic of the BIO tagging format, particularly for entity continuation (I-) tags. However, do not output any intermediate steps.\"\"\",\n",
    "        generation_config={\n",
    "        \"temperature\": 0.2\n",
    "        # Optionally: \"top_k\": ..., \"top_p\": ...\n",
    "    })\n",
    "    \n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/improved_prompt_gemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/improved_prompt_gemma.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla NO BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"Given entity label set: {{'O': 0, 'PERSON': 1, 'ORGANIZATION': 3, 'LOCATION': 5, 'MISCELLANEOUS': 7}}.\n",
    "            Based on the given entity label set, please recognize the named entities in the given text.\n",
    "            Text: '{tokens}'.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 0, 5, 5, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/vanilla_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/vanilla_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ner_confusion_matrix_from_csv(\"data_train_gemma/vanilla_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context aware NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "#for j in range(100):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"\n",
    "            Role: you are an expert in Named Entity Recognition (NER) and you are able to recognize entities in a sentence based on context.\n",
    "            Given the following NER tags: {{'O': 0, 'PERSON': 1, 'ORGANIZATION': 3, 'LOCATION': 5, 'MISCELLANEOUS': 7}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            Entity Type Profiles:\n",
    "            - PERSON: Names of individuals, titles with names, pronouns referring to people\n",
    "            - ORGANIZATION: Companies, institutions, government bodies, teams, bands\n",
    "            - LOCATION: Countries, cities, geographical features, addresses, buildings\n",
    "            - MISCELLANEOUS: Events, products, languages, nationalities, artworks\n",
    "\n",
    "            For each token, consider:\n",
    "            1. What type of entity could this be based on context?\n",
    "            2. What are the semantic clues around it?\n",
    "            3. Does it fit the entity profiles above?\n",
    "\n",
    "            The sentence is: '{tokens}'\n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 0, 5, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/context_aware.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/context_aware.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "#for j in range(100):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are performing Named Entity Recognition using the following tag IDs:  \n",
    "    - O: 0  \n",
    "    - PERSON: 1  \n",
    "    - ORGANIZATION: 3  \n",
    "    - LOCATION: 5  \n",
    "    - MISCELLANEOUS: 7\n",
    "    Example:  \n",
    "    Sentence: John works at Microsoft in Seattle  \n",
    "    Tokens: John, works, at, Microsoft, in, Seattle  \n",
    "    nertags: 1, 0, 0, 3, 0, 5\n",
    "    Now analyze this sentence: {tokens}  \n",
    "    Token count: {len(tokens)}\n",
    "    Your answer MUST follow the format: ner_tags: 0, 1, 0, 5, 5, 0\n",
    "    The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "    Do not include any other text or explanations.\n",
    "\"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/zs_example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/zs_example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "#for j in range(100):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"Assign NER tag IDs to each token in the sentence using this schema:  \n",
    "                O = 0, PERSON = 1, ORGANIZATION = 3, LOCATION = 5, MISCELLANEOUS = 7\n",
    "                First: extract the possible tokens from the sentence and then assign the NER tags to each token.\n",
    "                Second: cretate your POS tags for each token using the Viterbi algorithm and carefully consider the context of the sentence. After that, recontrol the POS tags and make sure they are correct.\n",
    "                Third: return the NER tags in the following format:\n",
    "                Your answer MUST follow the format: ner_tags: 0, 1, 0, 5, 5, 0\n",
    "                The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "                Do not include any other text or explanations.\n",
    "                Sentence: {tokens}  \n",
    "                Token count: {len(tokens)}\n",
    "                \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/lite_input_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/lite_input_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Prompt (strict, ID-based output, no explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "#for j in range(100):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in a sentence, based on the following tag schema:\n",
    "\n",
    "NER Tag IDs:\n",
    "- O: 0 (not an entity)\n",
    "- PERSON: 1 (names of people, personal titles)\n",
    "- ORGANIZATION: 3 (companies, teams, institutions)\n",
    "- LOCATION: 5 (cities, countries, landmarks, addresses)\n",
    "- MISCELLANEOUS: 7 (nationalities, products, events, languages, artworks)\n",
    "\n",
    "Use only the context to decide if a token refers to one of the entity types. Carefully consider surrounding tokens.\n",
    "\n",
    "Sentence: {tokens}  \n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "Your answer MUST follow the format: ner_tags: 0, 1, 0, 5, 5, 0\n",
    "The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence. Do not include any other text or explanations.\"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/context_based_prompt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/context_based_prompt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "#for j in range(100):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are a Named Entity Recognition (NER) expert. Assign an entity tag ID to each token in the sentence using the schema below.\n",
    "\n",
    "NER Tag IDs:\n",
    "- 0 = O: Not an entity\n",
    "- 1 = PERSON: Specific people or titles with names (e.g., \"Barack\", \"Smith\")\n",
    "- 3 = ORGANIZATION: Named institutions, companies, teams, or agencies (e.g., \"Google\", \"United Nations\")\n",
    "- 5 = LOCATION: Geographical locations or buildings (e.g., \"Paris\", \"Mount Everest\", \"Empire State Building\")\n",
    "- 7 = MISCELLANEOUS: Nationalities, events, products, languages, works of art (e.g., \"Italian\", \"Olympics\", \"iPhone\", \"French\")\n",
    "\n",
    "Warning: Only assign a tag if the token clearly matches an entity type based on context. If unsure, tag as `0 (O)`.\n",
    "\n",
    "---\n",
    "\n",
    "Example:\n",
    "\n",
    "Sentence: Barack Obama visited Microsoft in Seattle.  \n",
    "Tokens: Barack, Obama, visited, Microsoft, in, Seattle  \n",
    "ner_tags: 1, 1, 0, 3, 0, 5\n",
    "\n",
    "---\n",
    "\n",
    "Now tag the sentence below:\n",
    "\n",
    "Sentence: {tokens}  \n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "Your answer MUST follow this format:  \n",
    "ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "(Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "\"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/few_shot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/few_shot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ---\n",
    "\n",
    "# Example:\n",
    "# Sentence: Dr. Jane Smith gave a talk at Google in Berlin.  \n",
    "# Tokens: Dr., Jane, Smith, gave, a, talk, at, Google, in, Berlin  \n",
    "# ner_tags: 1, 1, 1, 0, 0, 0, 0, 3, 0, 5\n",
    "\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "#for j in range(100):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "            NER Tag IDs:\n",
    "            - 0 = O → Not an entity  \n",
    "            - 1 = PERSON → Real names, personal titles   \n",
    "            - 3 = ORGANIZATION → Companies, institutions, teams, agencies \n",
    "            - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "            - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works \n",
    "\n",
    "            ---\n",
    "\n",
    "            Rules:\n",
    "            - Use only contextual evidence to determine entity types.\n",
    "            - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "            - Please do not assign tags based on assumptions or incomplete context.\n",
    "            - Please be as precise as possible\n",
    "\n",
    "            ---\n",
    "\n",
    "            Now tag the sentence below:\n",
    "            Sentence: {tokens}  \n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Your answer MUST follow this format:  \n",
    "            ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "            (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "            \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/precision_enhanced_advanced_no_ex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/precision_enhanced_advanced_no_ex.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided method Luca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    pos_tags = sample_train_data[j]['pos_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "    NER Tag IDs:\n",
    "    - 0 = O → Not an entity  \n",
    "    - 1 = PERSON → Real names, personal titles \n",
    "    - 3 = ORGANIZATION → Companies, institutions, teams, agencies\n",
    "    - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "    - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works\n",
    "\n",
    "    ---\n",
    "\n",
    "    Rules:\n",
    "    - Use only contextual evidence to determine entity types.\n",
    "    - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "    - Please do not assign tags based on assumptions or incomplete context.\n",
    "    - Please be as precise as possible\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now tag the sentence below:\n",
    "    Sentence: {tokens}  \n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "    \n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Your answer MUST follow this format:  \n",
    "    ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "    (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "    \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/pos_guided_info_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/pos_guided_info_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ner_confusion_matrix_from_csv(\"data_train_gemma/pos_guided_no_bio_info_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sample_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sample_train_data[j]['tokens']\n",
    "    true_labels = sample_train_data[j]['ner_tags']\n",
    "    pos_tags = sample_train_data[j]['pos_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "    Given the following NER tags:\n",
    "    {{'O': 0, 'PER': 1, 'ORG': 3, 'LOC': 5, 'MISC': 7}}\n",
    "\n",
    "    Your task is to assign the correct tag number to each token in this sentence:\n",
    "    {tokens}\n",
    "\n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Respond ONLY with:\n",
    "    ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "    Do NOT include explanations, thoughts, or any other content.\n",
    "    Do NOT write anything before or after \"ner_tags: ...\".\n",
    "    Just print the sequence in the format specified.\n",
    "    \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/pos_guided_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\"data_train_gemma/pos_guided_no_bio.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insetti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
