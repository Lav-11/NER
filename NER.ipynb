{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (2.11.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (2.33.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Print a sample\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from the training data:\n",
      "{'id': '11001', 'tokens': ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.'], 'pos_tags': [11, 8, 15, 22, 22, 15, 22, 6, 22, 6, 22, 21, 10, 21, 15, 12, 16, 16, 22, 22, 21, 30, 38, 35, 22, 22, 22, 7], 'chunk_tags': [11, 12, 13, 11, 12, 13, 11, 0, 11, 0, 11, 12, 0, 11, 13, 11, 12, 12, 12, 12, 12, 3, 21, 13, 11, 12, 12, 0], 'ner_tags': [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]}\n",
      "Tokens: ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.']\n",
      "Labels: [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14041\n",
      "Validation set size: 3250\n",
      "Test set size: 3453\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "'''\n",
    "Store the response in a list of lists where the first element is the token, the second element \n",
    "is the predicted label and the third is the true label\n",
    "'''\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    assert (len(response_labels) == len(tokens)), \"Length of tokens and NER tags do not match\"\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        \n",
    "        assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens))]\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pydantic import BaseModel\n",
    "from google import genai\n",
    "\n",
    "for j in range(5):\n",
    "    tokens = train_data[j]['tokens']\n",
    "    true_labels = train_data[j]['ner_tags']\n",
    "    votes = [[] for _ in range(len(tokens))]\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    for i in range(5):\n",
    "        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        # Send request to Gemma\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "            The sentence is: '{tokens}'\n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Print only the number associated with the NER tag for each of the {len(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "            Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "            The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "            Do not include any other text or explanations.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        data = parse_response(tokens, response.text, true_labels)\n",
    "        #Â Store predicted labels for voting\n",
    "        store_predicted_labels([data[i][1] for i in range(len(data))], votes)\n",
    "    # Extract for each token the most voted label\n",
    "    votes = [max(set(vote), key=vote.count) for vote in votes]\n",
    "    save_to_csv(tokens, votes, true_labels, \"data/data_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
