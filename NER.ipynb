{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (2.11.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (2.33.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: seqeval in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.40.0)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.11.1)\n",
      "Requirement already satisfied: tqdm in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uritemplate, protobuf, httplib2, grpcio, proto-plus, googleapis-common-protos, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install pydantic\n",
    "!pip install seqeval\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Print a sample\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from the training data:\n",
      "{'id': '11001', 'tokens': ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.'], 'pos_tags': [11, 8, 15, 22, 22, 15, 22, 6, 22, 6, 22, 21, 10, 21, 15, 12, 16, 16, 22, 22, 21, 30, 38, 35, 22, 22, 22, 7], 'chunk_tags': [11, 12, 13, 11, 12, 13, 11, 0, 11, 0, 11, 12, 0, 11, 13, 11, 12, 12, 12, 12, 12, 3, 21, 13, 11, 12, 12, 0], 'ner_tags': [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]}\n",
      "Tokens: ['1886', '-', 'At', 'Skeleton', 'Canyon', 'in', 'Arizona', ',', 'Geronimo', ',', 'Apache', 'chief', 'and', 'leader', 'of', 'the', 'last', 'great', 'Red', 'Indian', 'rebellion', 'finally', 'surrendered', 'to', 'General', 'Nelson', 'Miles', '.']\n",
      "Labels: [0, 0, 0, 5, 6, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14041\n",
      "Validation set size: 3250\n",
      "Test set size: 3453\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        \n",
    "        assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens)) if pred_labels[i] != 0 and true_labels[i] != 0]\n",
    "    # Remove duplicates\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "{'id': '1645', 'tokens': ['12/15/2007', '850M', '4.85', '4.85'], 'pos_tags': [11, 11, 11, 11], 'chunk_tags': [11, 12, 12, 12], 'ner_tags': [0, 0, 0, 0]}\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "{'id': '986', 'tokens': ['Tambang', 'Timah', 'at', '$', '15.575', 'in', 'London', '.'], 'pos_tags': [39, 22, 15, 3, 11, 15, 22, 7], 'chunk_tags': [21, 11, 13, 11, 12, 13, 11, 0], 'ner_tags': [3, 4, 0, 0, 0, 0, 5, 0]}\n",
      "\n",
      "\n",
      "Sample 3:\n",
      "{'id': '529', 'tokens': ['SAN', 'FRANCISCO', '10', '3', '0', '325', '198'], 'pos_tags': [22, 22, 11, 11, 11, 11, 11], 'chunk_tags': [11, 12, 12, 12, 12, 12, 12], 'ner_tags': [3, 4, 0, 0, 0, 0, 0]}\n",
      "\n",
      "\n",
      "Sample 4:\n",
      "{'id': '302', 'tokens': ['The', 'West', 'Indies', 'vice-captain', 'struggled', 'for', 'timing', 'during', 'his', '36-minute', 'stay', 'at', 'the', 'crease', 'before', 'chipping', 'a', 'ball', 'from', 'medium', 'pacer', 'Tom', 'Moody', 'straight', 'to', 'Shane', 'Warne', 'at', 'mid-wicket', '.'], 'pos_tags': [12, 22, 22, 16, 38, 15, 21, 15, 29, 16, 21, 15, 12, 21, 15, 39, 12, 21, 15, 21, 21, 22, 22, 30, 35, 22, 22, 15, 22, 7], 'chunk_tags': [11, 12, 12, 12, 21, 13, 11, 13, 11, 12, 12, 13, 11, 12, 13, 21, 11, 12, 13, 11, 12, 12, 12, 3, 13, 11, 12, 13, 11, 0], 'ner_tags': [0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 0]}\n",
      "\n",
      "\n",
      "Sample 5:\n",
      "{'id': '3350', 'tokens': ['W', 'L', 'T', 'GF', 'GA', 'PTS'], 'pos_tags': [22, 22, 22, 22, 22, 22], 'chunk_tags': [11, 12, 12, 12, 12, 12], 'ner_tags': [0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample 300 random elements from the test set\n",
    "sampled_test_data = random.sample(list(test_data), 300)\n",
    "\n",
    "# Print the first few samples to verify\n",
    "for i, sample in enumerate(sampled_test_data[:5]):  # Display the first 5 samples\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(sample)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pydantic import BaseModel\n",
    "from google import genai\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    # tokens = train_data[j]['tokens']\n",
    "    # true_labels = train_data[j]['ner_tags']\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    votes = [[] for _ in range(len(tokens))]\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #for i in range(5):\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    # Send request to Gemma\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        The sentence is: '{tokens}'\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "        Print only the number associated with the NER tag for each of the {legn(tokens)} tokens, using the tag-to-number mapping provided above.\n",
    "        Your answer MUST follow the format: ner_tags: 0, 1, 2, 0, 0, 0\n",
    "        The number of output NER tags MUST be exactly {len(tokens)}, one for each token in the order they appear in the sentence.\n",
    "        Do not include any other text or explanations.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response(tokens, response.text, true_labels)\n",
    "        # # Store predicted labels for voting\n",
    "        # store_predicted_labels([data[i][1] for i in range(len(data))], votes)\n",
    "    # Extract for each token the most voted label\n",
    "    #votes = [max(set(vote), key=vote.count) for vote in votes]\n",
    "    #save_to_csv(tokens, votes, true_labels, \"data/data_test.csv\")\n",
    "    save_to_csv(tokens, [item[1] for item in data], true_labels, \"data/vanilla_test_300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# def save_basic_decomposed_qa_to_csv(tokens: list, pred_labels: dict, true_labels: dict, filename: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Save the token, predicted labels, and true labels to a CSV file for Basic Decomposed-QA.\n",
    "\n",
    "#     Args:\n",
    "#         tokens (list): List of tokens in the sentence.\n",
    "#         pred_labels (dict): Dictionary of predicted labels in the format {'PER': 'entity1, entity2', ...}.\n",
    "#         true_labels (dict): Dictionary of true labels in the format {'PER': 'entity1, entity2', ...}.\n",
    "#         filename (str): Path to the CSV file (relative to the 'data' folder).\n",
    "#     \"\"\"\n",
    "#     # Ensure the 'data' directory exists\n",
    "#     os.makedirs(\"data\", exist_ok=True)\n",
    "#     print(pred_labels)\n",
    "#     print(pred_labels['B-PER'])\n",
    "    \n",
    "#     # Full path to the file in the 'data' directory\n",
    "#     filepath = os.path.join(\"data\", filename)\n",
    "    \n",
    "#     file_exists = os.path.isfile(filepath)\n",
    "    \n",
    "#     # Write header only if the file didn't exist before\n",
    "#     if not file_exists:\n",
    "#         with open(filepath, 'a', newline='') as csvfile:\n",
    "#             header = ['token', 'pred', 'true']\n",
    "#             writer = csv.writer(csvfile)\n",
    "#             writer.writerow(header)\n",
    "    \n",
    "#     # Prepare data for each token\n",
    "#     data = []\n",
    "#     for token in tokens:\n",
    "#         predicted_label = None\n",
    "#         true_label = None\n",
    "        \n",
    "#         # Find the predicted label for the token\n",
    "#         for label, entities in pred_labels.items():\n",
    "#             if token in entities.split(', '):\n",
    "#                 predicted_label = label\n",
    "#                 break\n",
    "        \n",
    "#         # Find the true label for the token\n",
    "#         for label, entities in true_labels.items():\n",
    "#             if token in entities.split(', '):\n",
    "#                 true_label = label\n",
    "#                 break\n",
    "        \n",
    "#         # Append the token, predicted label, and true label\n",
    "#         data.append([token, predicted_label if predicted_label else 'O', true_label if true_label else 'O'])\n",
    "    \n",
    "#     # Write data to the CSV file\n",
    "#     with open(filepath, 'a', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerows(data)\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "def save_basic_decomposed_qa_to_csv(tokens: list, pred_labels: dict, true_labels: dict, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the token, predicted labels, and true labels to a CSV file for Basic Decomposed-QA.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of tokens in the sentence.\n",
    "        pred_labels (dict): Dictionary of predicted labels in the format {'PER': ['entity1', 'entity2'], ...}.\n",
    "        true_labels (dict): Dictionary of true labels in the format {'PER': ['entity1', 'entity2'], ...}.\n",
    "        filename (str): Path to the CSV file (relative to the 'data' folder).\n",
    "    \"\"\"\n",
    "    # Ensure the 'data' directory exists\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "    file_exists = os.path.isfile(filepath)\n",
    "\n",
    "    if not file_exists:\n",
    "        with open(filepath, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['token', 'pred', 'true'])\n",
    "\n",
    "    # Create reverse lookup from token to label index\n",
    "    def get_label_index(token_idx, label_dict):\n",
    "        for label_str, entities in label_dict.items():\n",
    "            if entities == 'None':\n",
    "                continue\n",
    "            tokens_in_label = [t.strip() for t in entities.split(',')]\n",
    "            if tokens[token_idx] in tokens_in_label:\n",
    "                # Convert label to index\n",
    "                for idx, tag in label_mapping.items():\n",
    "                    if tag == label_str:\n",
    "                        return idx\n",
    "        return 0  # 'O'\n",
    "\n",
    "    # Write data\n",
    "    with open(filepath, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i, token in enumerate(tokens):\n",
    "            pred_idx = get_label_index(i, pred_labels)\n",
    "            true_idx = get_label_index(i, true_labels)\n",
    "            writer.writerow([token, pred_idx, true_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m question = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: What are the named entities labeled as \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in the text? You only have to answer with the entity name, if there are multiple entities then separate them with a comma. If there are no entities, answer with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Finally, remember that a token can only be classified once.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Send the request to the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemma-3-27b-it\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcontext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Save the response and update the context\u001b[39;00m\n\u001b[32m     63\u001b[39m answer = response.text.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/google/genai/models.py:5049\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5047\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5048\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5049\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5052\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5053\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/google/genai/models.py:4025\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4022\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4023\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4025\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4026\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4030\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4031\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4032\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/google/genai/_api_client.py:751\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    742\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    743\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    746\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    747\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    748\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    749\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    750\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m   json_response = response.json\n\u001b[32m    753\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/google/genai/_api_client.py:673\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    669\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    670\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    671\u001b[39m   )\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m   errors.APIError.raise_for_response(response)\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    682\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    683\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/nlp/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Mapping of label numbers to readable tags\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# List of NER labels and their descriptions (without distinction between B- and I-)\n",
    "ner_labels = {\n",
    "    'PER': 'Person',\n",
    "    'ORG': 'Organization',\n",
    "    'LOC': 'Location',\n",
    "    'MISC': 'Miscellaneous'\n",
    "}\n",
    "\n",
    "# Iterate over the first 5 examples of the dataset\n",
    "for j in range(len(test_data)):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    votes = [[] for _ in range(len(tokens))]  # Initialize votes for majority voting\n",
    "\n",
    "    # Repeat the process 5 times for majority voting\n",
    "    #for iteration in range(5):\n",
    "    # Re-initialize the client for each sentence\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "    # Build a dictionary for the true labels\n",
    "    true_label_dict = {label: [] for label in ner_labels.keys()}\n",
    "    for token, true_label in zip(tokens, true_labels):\n",
    "        readable_label = label_mapping[true_label]\n",
    "        if readable_label != 'O':  # Ignore tokens without labels\n",
    "            main_label = readable_label.split('-')[-1]  # Get PER, ORG, LOC, MISC\n",
    "            true_label_dict[main_label].append(token)\n",
    "\n",
    "    # Convert lists to comma-separated strings\n",
    "    for key in true_label_dict:\n",
    "        true_label_dict[key] = ', '.join(true_label_dict[key]) if true_label_dict[key] else 'None'\n",
    "\n",
    "    context = f\"\"\"Given the following NER tags: {{'Person': 'PER', 'Organization': 'ORG', 'Location': 'LOC', 'Miscellaneaous': 'MISC'}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "    The sentence is: '{tokens}'.\"\"\"\n",
    "    results = []  # To save the model's responses\n",
    "    \n",
    "    # Iterate over each NER label\n",
    "    for label, description in ner_labels.items():\n",
    "        # Build the question for the current label\n",
    "        question = f\"Question: What are the named entities labeled as '{description}' in the text? You only have to answer with the entity name, if there are multiple entities then separate them with a comma. If there are no entities, answer with 'None'. Finally, remember that a token can only be classified once.\\n\"\n",
    "\n",
    "        # Send the request to the model\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemma-3-27b-it\",\n",
    "            contents=f\"{context}\\n{question}\\n\"\n",
    "        )\n",
    "\n",
    "        # Save the response and update the context\n",
    "        answer = response.text.strip()\n",
    "        results.append({label: answer})\n",
    "        context += f\"\\n{question}\\n{answer}\\n\"\n",
    "\n",
    "    # Parse the response and update votes\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        predicted_labels = []\n",
    "        for result in results:\n",
    "            label = list(result.keys())[0]  # Get the label (e.g., 'ORG')\n",
    "            predicted_entities = result[label].split(', ') if result[label] != 'None' else []\n",
    "            if token in predicted_entities:\n",
    "                predicted_labels.append(label)\n",
    "        # Add the predicted label for this token to the votes\n",
    "        votes[token_idx].append(predicted_labels[0] if predicted_labels else 'O')\n",
    "\n",
    "    # Perform majority voting for each token\n",
    "    final_labels = [max(set(vote), key=vote.count) for vote in votes]\n",
    "\n",
    "    # Build the final predicted labels in the desired format\n",
    "    final_label_dict = {label: [] for label in ner_labels.keys()}\n",
    "    for token, final_label in zip(tokens, final_labels):\n",
    "        if final_label != 'O':  # Ignore tokens without labels\n",
    "            final_label_dict[final_label].append(token)\n",
    "\n",
    "    # Convert lists to comma-separated strings\n",
    "    for key in final_label_dict:\n",
    "        final_label_dict[key] = ', '.join(final_label_dict[key]) if final_label_dict[key] else 'None'\n",
    "\n",
    "    # Print the results for the current sample\n",
    "    print(f\"Results for sample {j + 1}:\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"Predicted Labels:\")\n",
    "    print(final_label_dict)\n",
    "    print(\"True Labels:\")\n",
    "    print(true_label_dict)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    save_basic_decomposed_qa_to_csv(tokens, final_label_dict, true_label_dict, \"basic_decomposed_qa_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5513626834381551\n",
      "Recall: 0.6368038740920097\n",
      "F1 Score: 0.5910112359550561\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.56      0.86      0.68       139\n",
      "        MISC       0.43      0.29      0.35        41\n",
      "         ORG       0.26      0.23      0.24       118\n",
      "         PER       0.79      0.90      0.84       115\n",
      "\n",
      "   micro avg       0.55      0.64      0.59       413\n",
      "   macro avg       0.51      0.57      0.53       413\n",
      "weighted avg       0.53      0.64      0.57       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Define label mapping (inverse of your prompt's dictionary)\n",
    "index_to_label = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "# Step 2: Read the CSV and convert predictions and true labels\n",
    "true_seqs = []\n",
    "pred_seqs = []\n",
    "current_true = []\n",
    "current_pred = []\n",
    "\n",
    "with open('data/vanilla_test_300.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        token = row['token']\n",
    "        true_label = index_to_label[int(row['true'])]\n",
    "        pred_label = index_to_label[int(row['pred'])]\n",
    "\n",
    "        current_true.append(true_label)\n",
    "        current_pred.append(pred_label)\n",
    "\n",
    "        # Optional: if your dataset contains sentence boundaries, split sequences here\n",
    "        # For now we assume it's one sentence\n",
    "    true_seqs.append(current_true)\n",
    "    pred_seqs.append(current_pred)\n",
    "\n",
    "# Step 3: Compute metrics\n",
    "print(\"Precision:\", precision_score(true_seqs, pred_seqs))\n",
    "print(\"Recall:\", recall_score(true_seqs, pred_seqs))\n",
    "print(\"F1 Score:\", f1_score(true_seqs, pred_seqs))\n",
    "\n",
    "# Optional detailed report\n",
    "print(\"\\nDetailed classification report:\\n\")\n",
    "print(classification_report(true_seqs, pred_seqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PER I-PER\n",
      "Response:  None\n",
      "B-ORG I-ORG\n",
      "Response:  None\n",
      "B-LOC I-LOC\n",
      "Response:  None\n",
      "B-MISC I-MISC\n",
      "Response:  None\n",
      "B-PER I-PER\n",
      "Response:  None\n",
      "B-ORG I-ORG\n",
      "Response:  B-ORG : Tambang, I-ORG : Timah\n",
      "B-LOC I-LOC\n",
      "Response:  B-LOC : London\n",
      "B-MISC I-MISC\n",
      "Response:  None\n",
      "B-PER I-PER\n",
      "Response:  None\n",
      "B-ORG I-ORG\n",
      "Response:  None\n",
      "B-LOC I-LOC\n",
      "Response:  B-LOC : SAN\n",
      "I-LOC : FRANCISCO\n",
      "B-MISC I-MISC\n",
      "Response:  None\n",
      "B-PER I-PER\n",
      "Response:  B-PER : Tom, B-PER : Shane, I-PER : Moody, I-PER : Warne\n",
      "B-ORG I-ORG\n",
      "Response:  B-ORG : West, I-ORG : Indies\n",
      "B-LOC I-LOC\n",
      "Response:  None\n",
      "B-MISC I-MISC\n",
      "Response:  None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    120\u001b[39m     chat.send_message(intro_message)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIntro message failed, retrying in 30s:\u001b[39m\u001b[33m\"\u001b[39m, e)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Decomposed-QA\n",
    "#from google import genai\n",
    "# import google.generativeai as genai\n",
    "# import time\n",
    "\n",
    "\n",
    "# label_groups = [(1,2), (3,4), (5,6), (7,8)]\n",
    "# #labels_class = {'Person': 'PER', 'Organization': 'ORG', 'Location': 'LOC', 'Miscellaneaous': 'MISC'}\n",
    "\n",
    "# for j in range(len(sampled_test_data)):\n",
    "# #for j in range(6):\n",
    "#     tokens = sampled_test_data[j]['tokens']\n",
    "#     true_labels = sampled_test_data[j]['ner_tags']\n",
    "#     results = []\n",
    "#     # Configure with your API key\n",
    "#     genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "#     #genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "#     model = genai.GenerativeModel('gemma-3-27b-it')\n",
    "#     #model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "#     chat = model.start_chat() # No initial history\n",
    "#     intro_message = f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "#     The sentence is: '{tokens}'.\n",
    "#     This sentence contains exactly {len(tokens)} tokens.\n",
    "#     Remember that a token can only be classified once.\n",
    "#     Which are the tokens labeled as ’{label_mapping[group[0]]}’ and '{label_mapping[group[1]]} in the sentence? You only have to answer with entity name. If there are multiple tokens for a single category then separate the list of tokens with commas. If there are no entities, answer with 'None'\"\"\"\n",
    "#     try:\n",
    "#         chat.send_message(intro_message)\n",
    "#     except Exception as e:\n",
    "#         time.sleep(30)\n",
    "#         chat.send_message(intro_message)\n",
    "#     for group in label_groups[1:]:\n",
    "#         #print(label_mapping[group[0]], label_mapping[group[1]])\n",
    "#         try:\n",
    "#             response = chat.send_message(f\"\"\"Which are the tokens labeled as ’{label_mapping[group[0]]}’ and '{label_mapping[group[1]]}' in the text? You only have to answer with entity name and each entity must correspond to a token. If there are multiple tokens for a single category then separate the list of tokens with commas. If there are no entities, answer with 'None'\"\"\")\n",
    "#         except Exception as e:\n",
    "#             time.sleep(30)\n",
    "#             response = chat.send_message(f\"\"\"Which are the tokens labeled as ’{label_mapping[group[0]]}’ and '{label_mapping[group[1]]}' in the text? You only have to answer with entity name and each entity must correspond to a token. If there are multiple tokens for a single category then separate the list of tokens with commas. If there are no entities, answer with 'None'\"\"\")\n",
    "#         #print(response.text)\n",
    "#         answer = response.text.strip()\n",
    "#         results.append({group[1] : answer})\n",
    "#     print(\"Results\", results)\n",
    "    \n",
    "#     # Transform results into the pred_labels dictionary\n",
    "#     pred_labels = {label: [] for label in label_mapping.values()}\n",
    "#     for result in results:\n",
    "#         label = list(result.keys())[0]  \n",
    "#         entities = result[label].split(', ') if result[label] != 'None' else []\n",
    "#         pred_labels[label] = ', '.join(entities) if entities else 'None'\n",
    "\n",
    "#     # Transform true labels into the required dictionary format\n",
    "#     true_label_dict = {label: [] for label in label_mapping.values()}\n",
    "#     for token, true_label in zip(tokens, true_labels):\n",
    "#         readable_label = label_mapping[true_label]\n",
    "#         if readable_label != 'O':  \n",
    "#             # main_label = readable_label.split('-')[-1]\n",
    "#             # print(main_label)\n",
    "#             # true_label_dict[main_label].append(token)\n",
    "#             true_label_dict[readable_label].append(token)\n",
    "\n",
    "\n",
    "#     # Convert lists to comma-separated strings\n",
    "#     for key in true_label_dict:\n",
    "#         true_label_dict[key] = ', '.join(true_label_dict[key]) if true_label_dict[key] else 'None'\n",
    "\n",
    "#     #print(pred_labels)\n",
    "#     #print(true_label_dict)\n",
    "#     # Save the results to a CSV file\n",
    "#     save_basic_decomposed_qa_to_csv(tokens, pred_labels, true_label_dict, \"qa_results_bio.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "\n",
    "category_to_index = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "\n",
    "# label_mapping = {\n",
    "#     0: 'O',\n",
    "#     1: 'B-PER',\n",
    "#     2: 'I-PER',\n",
    "#     3: 'B-ORG',\n",
    "#     4: 'I-ORG',\n",
    "#     5: 'B-LOC',\n",
    "#     6: 'I-LOC',\n",
    "#     7: 'B-MISC',\n",
    "#     8: 'I-MISC'\n",
    "# }\n",
    "\n",
    "label_groups = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "#model = genai.GenerativeModel('gemma-3-27b-it')  # or 'gemini-2.0-flash'\n",
    "model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')  # or 'gemini-2.0-flash'\n",
    "\n",
    "#for j in range(len(sampled_test_data)):\n",
    "for j in range(50):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    results = []\n",
    "    \n",
    "    chat = model.start_chat()\n",
    "\n",
    "    # Send general instructions once\n",
    "    intro_message = (\n",
    "        f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "        The response should be for example: 'B-PER: entity1, entity2, entity3', 'I-PER: entity4, entity5'.\n",
    "        If there are no entities, answer with 'None', no further explanation is needed.\n",
    "        The sentence is: '{tokens}'.\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "        Remember that a token can only be classified once.\n",
    "        If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "        If there are no entities, just answer with 'None'.\"\"\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        chat.send_message(intro_message)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"Intro message failed, retrying in 30s:\", e)\n",
    "        time.sleep(30)\n",
    "        chat.send_message(intro_message)\n",
    "\n",
    "    # Delay between requests to avoid rate limit\n",
    "    for group in label_groups:\n",
    "        label_1 = label_mapping[group[0]]\n",
    "        label_2 = label_mapping[group[1]]\n",
    "        print(label_1, label_2)\n",
    "        prompt = (\n",
    "            f\"\"\"Which are the tokens labeled as '{label_1}' and '{label_2}' in the text? \"\"\"\n",
    "            f\"\"\"You only have to answer with category : entity name and each entity must correspond to a token. \"\"\"\n",
    "            \n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = chat.send_message(prompt)\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed for {label_1}/{label_2}, retrying in 30s:\", e)\n",
    "            time.sleep(30)\n",
    "            response = chat.send_message(prompt)\n",
    "        print('Response: ', response.text)\n",
    "        #time.sleep(2)  # Small delay to reduce request rate\n",
    "        answer = response.text.strip()\n",
    "        results.append({label_mapping[group[1]]: answer})  # Use label string directly\n",
    "\n",
    "    # Build predicted labels as token lists\n",
    "    pred_labels = {label: [] for label in label_mapping.values()}\n",
    "    for result in results:\n",
    "        label = list(result.keys())[0]\n",
    "        entities = result[label].split(', ') if result[label] != 'None' else []\n",
    "        pred_labels[label] = entities\n",
    "\n",
    "    # Build true label dictionary\n",
    "    true_label_dict = {label: [] for label in label_mapping.values()}\n",
    "    for token, true_label in zip(tokens, true_labels):\n",
    "        label_str = label_mapping[true_label]\n",
    "        if label_str != 'O':\n",
    "            true_label_dict[label_str].append(token)\n",
    "\n",
    "    # Convert to strings for storage\n",
    "    for key in true_label_dict:\n",
    "        true_label_dict[key] = ', '.join(true_label_dict[key]) if true_label_dict[key] else 'None'\n",
    "    for key in pred_labels:\n",
    "        pred_labels[key] = ', '.join(pred_labels[key]) if pred_labels[key] else 'None'\n",
    "\n",
    "    save_basic_decomposed_qa_to_csv(tokens, pred_labels, true_label_dict, \"qa_results_bio_flash_25.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7281962411375407\n",
      "Recall: 0.8122881731878271\n",
      "F1 Score: 0.7430039993271146\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.87      0.80        39\n",
      "        MISC       0.11      0.50      0.18        10\n",
      "           O       0.99      0.92      0.96       578\n",
      "         ORG       0.83      0.77      0.80        39\n",
      "         PER       0.97      1.00      0.98        60\n",
      "\n",
      "    accuracy                           0.91       726\n",
      "   macro avg       0.73      0.81      0.74       726\n",
      "weighted avg       0.96      0.91      0.93       726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with open('data/qa_results.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        true_labels.append(row['true'])\n",
    "        pred_labels.append(row['pred'])\n",
    "\n",
    "# Compute token-level metrics\n",
    "print(\"Precision:\", precision_score(true_labels, pred_labels, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(true_labels, pred_labels, average='macro', zero_division=0))\n",
    "print(\"F1 Score:\", f1_score(true_labels, pred_labels, average='macro', zero_division=0))\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(true_labels, pred_labels, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
