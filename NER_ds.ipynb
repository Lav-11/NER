{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea07dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install pydantic\n",
    "!pip install seqeval\n",
    "!pip install google-generativeai\n",
    "!pip install datasets[all]\n",
    "!pip install pandas\n",
    "!pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample from the training data\n",
    "example = train_data[11001]\n",
    "print(\"Example from the training data:\")\n",
    "print(example)\n",
    "print(\"Tokens:\", example[\"tokens\"])\n",
    "print(\"Labels:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48761050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        \n",
    "        assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv_vanilla(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens)) if pred_labels[i] != 0 or true_labels[i] != 0]\n",
    "    # Remove duplicates\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predicted labels and for each token save the label in a list to be used for voting\n",
    "def store_predicted_labels(pred_labels : list, votes : list) -> None:\n",
    "    for i in range(len(pred_labels)):\n",
    "        votes[i].append(pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Sample 300 random elements from the test set\n",
    "sampled_test_data = random.sample(list(test_data), 300)\n",
    "\n",
    "# Print the first few samples to verify\n",
    "for i, sample in enumerate(sampled_test_data[:5]):  # Display the first 5 samples\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(sample)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "dependency_trees = []\n",
    "\n",
    "for sample in sampled_test_data:\n",
    "    tokens = sample[\"tokens\"]\n",
    "    sentence = \" \".join(tokens)\n",
    "    doc = nlp(sentence)\n",
    "    tree = [\n",
    "        {\n",
    "            \"text\": token.text,\n",
    "            \"dep\": token.dep_,\n",
    "            \"head\": token.head.text,\n",
    "            \"pos\": token.pos_,\n",
    "            \"index\": token.i,\n",
    "            \"head_index\": token.head.i\n",
    "        }\n",
    "        for token in doc\n",
    "    ]\n",
    "    dependency_trees.append(tree)\n",
    "\n",
    "# Link the two lists\n",
    "for i, sample in enumerate(sampled_test_data):\n",
    "    sample['dependency_tree'] = dependency_trees[i]\n",
    "\n",
    "# Now each sample has the tree inside\n",
    "print(sampled_test_data[0]['dependency_tree'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "print(dir(ollama))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "models = client.list()\n",
    "print(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b653c",
   "metadata": {},
   "source": [
    "********************************************************************************************************************\n",
    "Vanilla\n",
    "\n",
    "You are a strict NER tagging system.\n",
    "\n",
    "Given the following NER tags:\n",
    "{{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "Your task is to assign the correct tag number to each token in this sentence:\n",
    "{tokens}\n",
    "\n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "Respond ONLY with:\n",
    "ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "Do NOT include explanations, thoughts, or any other content.\n",
    "Do NOT write anything before or after \"ner_tags: ...\".\n",
    "Just print the sequence in the format specified.\n",
    "********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbc2c2",
   "metadata": {},
   "source": [
    "Word-level named entity reflection (doesn't work well)\n",
    "\n",
    "You are a strict NER tagging system.\n",
    "\n",
    "Given the following NER tags:\n",
    "{{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "Your task is to assign the correct tag number to each token in this sentence:\n",
    "{tokens}\n",
    "\n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "For each word in the text, generate a short summary (maximum 10 words) reasoning about its possible named entity category.\n",
    "Respond ONLY with:\n",
    "1)explanation for each word.\n",
    "2)ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "Just print the sequence in the format specified.\n",
    "********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c41b90",
   "metadata": {},
   "source": [
    "\n",
    "Multi-turn adaptive refinement (works a bit better than simple vanilla)\n",
    "\n",
    "You are a strict NER tagging system.\n",
    "\n",
    "Given the following NER tags:\n",
    "{{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "Your task is to assign the correct tag number to each token in this sentence:\n",
    "{tokens}\n",
    "\n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "First, extract potential named entities, then refine the list by validating their relationships within the text.\n",
    "Finally, consolidate the results.\n",
    "\n",
    "Respond ONLY with:\n",
    "1)the explanation as indicated above.\n",
    "2)ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "Just print the sequence in the format specified.\n",
    "********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef18068",
   "metadata": {},
   "source": [
    "\n",
    "Dependency-based entity validation (doesn't work well)\n",
    "\n",
    "You are a strict NER tagging system.\n",
    "\n",
    "Given the following NER tags:\n",
    "{{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "Your task is to assign the correct tag number to each token in this sentence:\n",
    "{tokens}\n",
    "\n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "You are also given the dependency tree of the sentence in the following format:\n",
    "{dependency_tree_str}\n",
    "\n",
    "First, extract potential named entities based on the tokens.\n",
    "Then, refine and validate the entities by analyzing their syntactic relationships according to the dependency tree.\n",
    "Finally, consolidate the results into a final tagging.\n",
    "\n",
    "Respond ONLY with:\n",
    "ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "Just print the sequence in the format specified.\n",
    "********************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from ollama import Client\n",
    "\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "def clean_response(text):\n",
    "    \"\"\"Cleans the model's output to extract only the numbers.\"\"\"\n",
    "    # Remove <think>...</think> blocks\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Look for the pattern 'ner_tags: 0, 1, 2, ...'\n",
    "    match = re.search(r'ner_tags\\s*:\\s*([0-9,\\s]+)', cleaned)\n",
    "    if match:\n",
    "        number_str = match.group(1)\n",
    "    else:\n",
    "        # If 'ner_tags:' is not found, but there are still numbers, extract them all\n",
    "        number_str = cleaned\n",
    "\n",
    "    # Extract all integers as strings\n",
    "    number_list = re.findall(r'\\d+', number_str)\n",
    "    return number_list\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    dependency_tree = dependency_trees[j]\n",
    "\n",
    "    # Convert dependency_tree to JSON string\n",
    "    dependency_tree_str = json.dumps(dependency_tree, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "Given the following NER tags:\n",
    "{{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "Your task is to assign the correct tag number to each token in this sentence:\n",
    "{tokens}\n",
    "\n",
    "This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "You are also given the dependency tree of the sentence in the following format:\n",
    "{dependency_tree_str}\n",
    "\n",
    "First, extract potential named entities based on the tokens.\n",
    "Then, refine and validate the entities by analyzing their syntactic relationships according to the dependency tree.\n",
    "Finally, consolidate the results into a final tagging.\n",
    "\n",
    "Respond ONLY with:\n",
    "ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "Just print the sequence in the format specified.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        if len(pred_tags_str) != len(tokens):\n",
    "            raise ValueError(f\"Error at sentence {j}: Number of labels ({len(pred_tags_str)}) does not match number of tokens ({len(tokens)})\\nRaw response: {raw_text}\")\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data/vanilla_test_300_ds_14b_DBEVS.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e036d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
