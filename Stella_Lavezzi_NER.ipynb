{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2451a21b",
   "metadata": {},
   "source": [
    "## Master degree in Computer Engineering (Ai & Robotics)\n",
    "\n",
    "**Stella Francesco 2124359** \\\n",
    "**Lavezzi Luca 2154256**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7e672",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "In this notebook we address the task of Named-Entity Recognition (NER), which consists in identifying and classifying entities such as persons, organizations, locations, and miscellaneous names within text. NER is a fundamental problem in Natural Language Processing (NLP), with applications ranging from information extraction to question answering.\n",
    "\n",
    "For our experiments, we use the widely adopted CoNLL-2003 dataset, which contains annotated newswire articles with four types of named entities: persons, organizations, locations, and miscellaneous. This dataset is a standard benchmark for evaluating NER systems in the general domain.\n",
    "\n",
    "To explore zero(few)-shot NER, we experiment with two instruction-tuned large language models (LLMs): **Gemma 3-27b-it**, which is accessed via API, and **DeepSeek-R1:14b**, which is executed locally using the Ollama framework.\n",
    "\n",
    "Following the guidelines of the project, we implement and evaluate the baseline method from Xie et al. (2023), as well as one additional zero-shot prompting strategy inspired by their work. We refine our prompts and methods using a training split of the dataset, and report results on a separate test split, following best practices for NER evaluation.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "- Introduction of the dataset and domain\n",
    "- Documentation of any additional annotation or external libraries used\n",
    "- Introduction to the models used\n",
    "- Description of each zero(few)-shot method\n",
    "- Evaluation and critical discussion of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de734ab",
   "metadata": {},
   "source": [
    "# CoNLL 2003 Dataset\n",
    "\n",
    "For this project, we used the CoNLL-2003 shared task dataset on language-independent named entity recognition (NER). This dataset includes annotated data for two languages, German and English, but we exclusively used the English portion for training and testing.\n",
    "\n",
    "The English data is sourced from the Reuters Corpus, comprising newswire articles published between August 1996 and August 1997. Specifically, the training set consists of articles from a ten-day period toward the end of August 1996, while the test set includes articles from December 1996.\n",
    "\n",
    "Each token in the dataset is annotated with one of the following four named entity types:\n",
    "\n",
    "* PER – Person\n",
    "* ORG – Organization\n",
    "* LOC – Location\n",
    "* MISC – Miscellaneous entities not covered by the other categories\n",
    "* O – Outside of a named entity\n",
    "\n",
    "The CoNLL-2003 dataset is widely regarded as a benchmark for evaluating NER systems due to its standardized format, high-quality annotations, and well-defined evaluation protocol (typically based on precision, recall, and F1-score). It continues to serve as a critical resource for comparing both traditional machine learning and modern deep learning approaches in sequence labeling tasks.\n",
    "\n",
    "Using the Hugging Face Datasets library, we imported the dataset in JSON format, where each instance contains the following fields:\n",
    "\n",
    "* id – The identifier of the sample\n",
    "* tokens – A list containing the tokenized sentence\n",
    "* pos_tags – A list of part-of-speech (POS) tags associated with each token\n",
    "* chunk_tags – A list of syntactic chunk tags\n",
    "* ner_tags – A list of named entity recognition tags corresponding to the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975fa0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: {'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n",
      "\n",
      "Training set size: 14041\n",
      "Validation set size: 3250\n",
      "Test set size: 3453\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(0)\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Sample\n",
    "print(\"Example:\", train_data[0])\n",
    "print(\"\\nTraining set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a098f6",
   "metadata": {},
   "source": [
    "For both the training and testing phases, we used seed 0 to ensure reproducibility, and the samples were taken from the training and test splits provided by the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b7960",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "## Google library\n",
    "The Google Gen AI Python SDK provides an interface for integrating Google's generative models into Python applications. We used this library because we tested our prompts using Gemma 3. The SDK offers access to a variety of large language models from Google, through a user-friendly structure and flexible configuration options. It supports features such as adjusting the temperature for response creativity and sending batch requests for more efficient processing.\n",
    "\n",
    "## Hugging Face\n",
    "Hugging Face is a leading machine learning and data science platform and community that enables users to build, train, and deploy machine learning models with ease. It offers a wide range of pretrained large language models (LLMs) that can be used for tasks such as text generation, summarization, translation, classification, and more.\n",
    "\n",
    "Developers can also fine-tune these models on custom datasets or even build new models from scratch using the tools provided by Hugging Face. A key component of the platform is the Transformers library, which we used in this project. It provides seamless integration with popular deep learning frameworks like PyTorch and TensorFlow, making it easier to experiment and scale models across different environments.\n",
    "\n",
    "In addition to models, Hugging Face hosts datasets such as CoNLL2003.\n",
    "\n",
    "## SeqEval\n",
    "Seqeval is a Python library designed for evaluating sequence labeling tasks, such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and chunking. It provides a simple and efficient way to compute standard evaluation metrics, including precision, recall, and F1-score, specifically tailored for structured prediction tasks where labels follow formats like IOB/IOB2. We adopt this library to evaluate our results and compare the various methods.\n",
    "\n",
    "## Ollama\n",
    "The ollama library provides a Python interface to interact with large language models (LLMs) running locally via the Ollama server. It allows users to send prompts, receive model-generated responses, and manage inference workflows directly from Python code. This makes it easy to integrate advanced generative AI capabilities into data analysis pipelines, research experiments, or application development, all while keeping computation on the local machine.\n",
    "\n",
    "## SpaCy\n",
    "The SpaCy library is an open-source software library for advanced natural language processing (NLP), written in Python and Cython. Unlike libraries such as NLTK, which are often used for teaching and research, spaCy is designed specifically for production use, offering fast and robust tools for tasks like tokenization, part-of-speech tagging, dependency parsing, text categorization, and named entity recognition (NER).\n",
    "\n",
    "spaCy supports deep learning workflows and can integrate with popular machine learning libraries such as TensorFlow and PyTorch via its own backend, Thinc. It provides prebuilt neural network models for 23 languages and supports tokenization for over 65 languages, making it suitable for both multilingual applications and custom model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7e6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seqeval in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-generativeai in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.25.0rc1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.40.0)\n",
      "Requirement already satisfied: protobuf in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (2.11.1)\n",
      "Requirement already satisfied: tqdm in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipywidgets) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/francescostella/anaconda3/envs/nlp/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U google-genai\n",
    "%pip install seqeval\n",
    "%pip install google-generativeai\n",
    "%pip install ipywidgets --upgrade\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ea74f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (2.11.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets[all] in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from datasets[all]) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from datasets[all]) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (0.31.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from datasets[all]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from datasets[all]) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets[all]) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests>=2.32.2->datasets[all]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests>=2.32.2->datasets[all]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests>=2.32.2->datasets[all]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests>=2.32.2->datasets[all]) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from tqdm>=4.66.3->datasets[all]) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pandas->datasets[all]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pandas->datasets[all]) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[all]) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets[all]) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: datasets 3.6.0 does not provide the extra 'all'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (0.4.8)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from ollama) (2.11.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (0.15.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucal\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lucal\\.conda\\envs\\insetti\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 2.6/12.8 MB 54.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 77.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.3/12.8 MB 110.0 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 93.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 65.5 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\lucal\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\lucal\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\lucal\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\lucal\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\lucal\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "C:\\Users\\lucal\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "WARNING: Skipping c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\lucal\\.conda\\envs\\insetti\\Lib\\site-packages\\scipy-1.13.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic\n",
    "!pip install datasets[all]\n",
    "!pip install pandas\n",
    "!pip install ollama\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425123a",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For the development and evaluation of the prompts in this project, we used two different large language models: Gemma 3 (27B) and DeepSeek.\n",
    "\n",
    "## Gemma 3 (27b)\n",
    "\n",
    "We chose Gemma 3 because it is a recently released model by Google, offering several notable advantages:\n",
    "\n",
    "* Computational efficiency: Gemma 3 is a relatively lightweight model that achieves reasonable performance compared to state-of-the-art (SOTA) models like Gemini 2.5 Pro, which has over ten times more parameters.\n",
    "* API limits: Unlike many other models with free-tier APIs, Gemma 3 allows for a higher number of requests per day, making it more suitable for iterative development and testing.\n",
    "\n",
    "However, these advantages come with a significant trade-off:\n",
    "\n",
    "* Lower performance: In our tests, Gemma 3 underperformed compared to more advanced models. When evaluated against Gemini 2.0 Flash, we observed a performance gap of up to 20% in F1-score. Despite this, using higher-performing models was impractical due to API rate limitations, which would have required a considerable amount of additonal time to complete equivalent testing. \n",
    "\n",
    "The lower accuracy of Gemma 3 also influenced our implementation: model outputs required additional validation and post-processing. In some cases, responses did not adhere to the expected output format, necessitating extra checks and error handling in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613880d",
   "metadata": {},
   "source": [
    "## DeepSeek-R1 (distilled)\n",
    "In this section, we show and discuss our implementation on the DeepSeek-R1 model (distilled).\n",
    "\n",
    "### How to run DeepSeek-R1 locally\n",
    "Go to https://ollama.com, download and install ollama.\n",
    "Then, search for DeepSeek-R1's available models: https://ollama.com/library/deepseek-r1.\n",
    "Click on a model you wish to download and copy the command to run in the terminal (for instance: ollama run deepseek-r1:14b).\n",
    "\n",
    "\n",
    "### Model selection\n",
    "Our choice for the model was based on two factors: time consumption and performance.\n",
    "We decided to use deepseek-r1:14b because it generated tokens relatively fast while still performing almost as good as gemma-3-27b-it, a model we used previously which had acceptable performance.\n",
    "Trying to run bigger models such as deepseek-r1:32b was not feasible due to the very long token generation times.\n",
    "The increase in performance compared to deepseek-r1:14b was also very minimal, which led us to try even smaller models, such as deepseek-r1:7b and deepseek-r1:8b, which performed much worse instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87739fc",
   "metadata": {},
   "source": [
    "## **Implemented methods**\n",
    "In this section, we elencate and explain our methods implemented using deepseek-r1:14b.\n",
    "\n",
    "### - Word-level named entity reflection\n",
    "\n",
    "In this method, we add to the vanilla prompt another instruction which asks the model to generate a short summary for each word in the sentence. The summary had to be very short (around ten words) and is supposed to give an explanation, motivating the reason why a certain word can or cannot be a potential candidate to be classified as a NER tag.\n",
    "\n",
    "This approach explicitly leverages the **chain of thought** capability of large language models: by prompting the model to reflect and reason about each token individually, it is encouraged to articulate its decision-making process step by step. This not only helps the model to make more informed tagging decisions, but also provides interpretability, as the generated summaries reveal the rationale behind each prediction.\n",
    "\n",
    "However, this method performed slightly worse compared to the baseline reported in the reference paper.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "### - Multi-turn adaptive refinement\n",
    "\n",
    "In this method we asked the model to name the potential candidate words to be classified as NER tags. The model had to give an explanation for every word it believed to be a candidate NER tag, and then give the results in the correct format.\n",
    "\n",
    "Similarly to the previously mentioned method, this approach explicitly leverages the **chain of thought** capabilities of large language models. By prompting the model to reason step by step about which words are likely entities and to justify its choices, we encourage a more structured and interpretable decision-making process. This not only helps the model focus on the most relevant tokens, but also reduces the overall size of the output text generated, since the prompt specifies to just list the candidate NER tags and their explanations.\n",
    "\n",
    "This method performed slightly better compared to the baseline counterpart.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "### - Dependency-based entity validation\n",
    "In this method we added the dependency tree in the prompt, obtained by using spaCy (the dependency tree was not part of the dataset).\n",
    "\n",
    "The motivation behind this approach was to provide the model with additional syntactic information about the relationships between words in the sentence. By including the dependency tree, the model could potentially leverage the grammatical structure to better identify entities, especially in complex sentences where context and word dependencies play a crucial role.\n",
    "\n",
    "To implement this, we used spaCy to generate the dependency tree for each sentence, representing the syntactic relations in a structured format. This tree was then inserted into the prompt alongside the tokens to be tagged. The expectation was that the model, with access to this extra layer of linguistic information, would be able to make more informed tagging decisions, particularly in cases where surface-level cues were insufficient.\n",
    "\n",
    "However, in our experiments, we observed that the F1 score associated to this method was slightly worse.\n",
    "\n",
    "It is also important to note that the dependency trees were not originally part of the dataset and had to be computed using spaCy. This introduces a potential source of error, as the quality of the dependency parsing depends on the accuracy of spaCy's model. If the dataset had included gold-standard dependency trees, or if the dependency information had been annotated and curated specifically for the NER task, the results could have been better and the model might have been able to exploit this information more effectively.\n",
    "\n",
    ">This method was already used in the reference paper.\n",
    "\n",
    "### - POS-guided named entity recognition\n",
    "In this method we added the POS tags for every token of the sentence, which are part of the dataset.\n",
    "\n",
    "The main idea behind this approach is to provide the model with explicit information about the grammatical role of each token in the sentence. Part-of-speech (POS) tags indicate whether a word is a noun, verb, adjective, proper noun, etc., and can be very helpful for named entity recognition because certain entity types are strongly associated with specific POS tags (for example, proper nouns are often persons, organizations, or locations).\n",
    "\n",
    "By including the POS tags in the prompt, we aimed to help the model disambiguate cases where the token alone might not be sufficient to determine the correct entity type. For instance, the word \"Apple\" could refer to a fruit (common noun) or a company (proper noun), and the POS tag can provide a useful clue for the model to make the right choice.\n",
    "\n",
    "In practice, we formatted the prompt so that the model received both the list of tokens and the corresponding POS tags in order. This additional information allowed the model to make more informed predictions, especially in sentences with ambiguous or rare words. Our experiments showed that this method generally improved the F1 score compared to the vanilla approach, confirming the usefulness of syntactic information for NER tasks.\n",
    "\n",
    "The performance scores indicate that this method was better than the baseline method implemented in the reference paper.\n",
    "\n",
    ">This method was already used in the reference paper.\n",
    "\n",
    "### - POS-dependency hybrid NER\n",
    "In this method we added to the prompt both the POS tags and the dependency tree.\n",
    "\n",
    "The rationale behind this approach is to combine two complementary sources of linguistic information: the part-of-speech (POS) tags, which indicate the grammatical role of each token, and the dependency tree, which describes the syntactic relationships between words in the sentence. By providing both types of annotations, we aimed to give the model a richer context for making NER predictions, especially in cases where either POS or dependency information alone might not be sufficient.\n",
    "\n",
    "In practice, the prompt was structured to include the list of tokens, their corresponding POS tags, and the full dependency tree (as computed by spaCy) for each sentence. This allowed the model to consider not only the type of each word but also how words are connected and which tokens are likely to form multi-word entities based on their syntactic structure.\n",
    "\n",
    "However, our experiments showed that the performance was close to the dependency-based entity validation method. Additionally, as with the dependency-based method, the quality of the dependency tree depends on the accuracy of spaCy's parser. If gold-standard dependency annotations had been available in the dataset, the results might have been better and the model could have leveraged this information more effectively.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "### - Example-driven POS NER\n",
    "In this method we added the POS tags for every token of the sentence. We also added three complete examples, which consisted of the sentence tokens, POS tags and NER tags associated to such tokens. Thus this can be considered as a **few-shot learning** method.\n",
    "\n",
    "The main motivation for this approach was to provide the model with concrete, context-rich demonstrations of the NER task, making it easier for the model to generalize the tagging strategy to new sentences. By including three full examples in the prompt—each showing the tokens, their corresponding POS tags, and the correct NER tags—the model could observe how the tagging should be performed in practice, even for more complex or ambiguous cases.\n",
    "\n",
    "In our experiments, this method generally led to similar performance compared to the original prompt without examples.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e870c7",
   "metadata": {},
   "source": [
    "## **Confrontation between our models, prompt switching and prompt mixing**\n",
    "After evaluating the various prompts for our models, we chose to focus on those approaches that provided the highest F1 scores.\n",
    "The next step consisted in applying the best-performing prompt from one model to the other model. In this way, we could directly compare whether the improvements were due to the prompt itself or to the specific model architecture.\n",
    "\n",
    "In particular, we took the prompt that gave the best results with the first model and used it as input for the second model, and vice versa. This allowed us to assess the transferability and general effectiveness of each prompt, independently of the model for which it was originally designed.\n",
    "\n",
    "In the end, we also experimented with combining the two best prompts, integrating their most effective elements into a single hybrid prompt. This \"prompt mixing\" approach aimed to leverage the strengths of both strategies, further improving the overall performance and robustness of our NER system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f918d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insetti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
