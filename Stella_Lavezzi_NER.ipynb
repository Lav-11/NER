{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2451a21b",
   "metadata": {},
   "source": [
    "## Master degree in Computer Engineering (Ai & Robotics)\n",
    "\n",
    "**Stella Francesco 2124359** \\\n",
    "**Lavezzi Luca 2154256**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7e672",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "In this notebook we address the task of Named-Entity Recognition (NER), which consists in identifying and classifying entities such as persons, organizations, locations, and miscellaneous names within text. NER is a fundamental problem in Natural Language Processing (NLP), with applications ranging from information extraction to question answering.\n",
    "\n",
    "For our experiments, we use the widely adopted CoNLL-2003 dataset, which contains annotated newswire articles with four types of named entities: persons, organizations, locations, and miscellaneous. This dataset is a standard benchmark for evaluating NER systems in the general domain.\n",
    "\n",
    "To explore zero(few)-shot NER, we experiment with two instruction-tuned large language models (LLMs): **Gemma 3-27b-it**, which is accessed via API, and **DeepSeek-R1:14b**, which is executed locally using the Ollama framework.\n",
    "\n",
    "Following the guidelines of the project, we implement and evaluate the baseline method from Xie et al. (2023), as well as one additional zero-shot prompting strategy inspired by their work. We refine our prompts and methods using a training split of the dataset, and report results on a separate test split, following best practices for NER evaluation.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "- Introduction of the dataset and domain\n",
    "- Documentation of any additional annotation or external libraries used\n",
    "- Introduction to the models used\n",
    "- Description of each zero(few)-shot method\n",
    "- Evaluation and critical discussion of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de734ab",
   "metadata": {},
   "source": [
    "# CoNLL 2003 Dataset\n",
    "\n",
    "For this project, we used the CoNLL-2003 shared task dataset on language-independent named entity recognition (NER). This dataset includes annotated data for two languages, German and English, but we exclusively used the English portion for training and testing.\n",
    "\n",
    "The English data is sourced from the Reuters Corpus, comprising newswire articles published between August 1996 and August 1997. Specifically, the training set consists of articles from a ten-day period toward the end of August 1996, while the test set includes articles from December 1996.\n",
    "\n",
    "Each token in the dataset is annotated with one of the following four named entity types:\n",
    "\n",
    "* PER – Person\n",
    "* ORG – Organization\n",
    "* LOC – Location\n",
    "* MISC – Miscellaneous entities not covered by the other categories\n",
    "* O – Outside of a named entity\n",
    "\n",
    "The CoNLL-2003 dataset is widely regarded as a benchmark for evaluating NER systems due to its standardized format, high-quality annotations, and well-defined evaluation protocol (typically based on precision, recall, and F1-score). It continues to serve as a critical resource for comparing both traditional machine learning and modern deep learning approaches in sequence labeling tasks.\n",
    "\n",
    "Using the Hugging Face Datasets library, we imported the dataset in JSON format, where each instance contains the following fields:\n",
    "\n",
    "* id – The identifier of the sample\n",
    "* tokens – A list containing the tokenized sentence\n",
    "* pos_tags – A list of part-of-speech (POS) tags associated with each token\n",
    "* chunk_tags – A list of syntactic chunk tags\n",
    "* ner_tags – A list of named entity recognition tags corresponding to the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975fa0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(0)\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"eriktks/conll2003\")\n",
    "\n",
    "# Access the train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Sample\n",
    "print(\"Example:\", train_data[0])\n",
    "print(\"\\nTraining set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(validation_data))\n",
    "print(\"Test set size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a098f6",
   "metadata": {},
   "source": [
    "For both the training and testing phases, we used seed 0 to ensure reproducibility, and the samples were taken from the training and test splits provided by the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b7960",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "## Google library\n",
    "The Google Gen AI Python SDK provides an interface for integrating Google's generative models into Python applications. We used this library because we tested our prompts using Gemma 3. The SDK offers access to a variety of large language models from Google, through a user-friendly structure and flexible configuration options. It supports features such as adjusting the temperature for response creativity and sending batch requests for more efficient processing.\n",
    "\n",
    "## Hugging Face\n",
    "Hugging Face is a leading machine learning and data science platform and community that enables users to build, train, and deploy machine learning models with ease. It offers a wide range of pretrained large language models (LLMs) that can be used for tasks such as text generation, summarization, translation, classification, and more.\n",
    "\n",
    "Developers can also fine-tune these models on custom datasets or even build new models from scratch using the tools provided by Hugging Face. A key component of the platform is the Transformers library, which we used in this project. It provides seamless integration with popular deep learning frameworks like PyTorch and TensorFlow, making it easier to experiment and scale models across different environments.\n",
    "\n",
    "In addition to models, Hugging Face hosts datasets such as CoNLL2003.\n",
    "\n",
    "## SeqEval\n",
    "Seqeval is a Python library designed for evaluating sequence labeling tasks, such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and chunking. It provides a simple and efficient way to compute standard evaluation metrics, including precision, recall, and F1-score, specifically tailored for structured prediction tasks where labels follow formats like IOB/IOB2. We adopt this library to evaluate our results and compare the various methods.\n",
    "\n",
    "## Ollama\n",
    "The ollama library provides a Python interface to interact with large language models (LLMs) running locally via the Ollama server. It allows users to send prompts, receive model-generated responses, and manage inference workflows directly from Python code. This makes it easy to integrate advanced generative AI capabilities into data analysis pipelines, research experiments, or application development, all while keeping computation on the local machine.\n",
    "\n",
    "## SpaCy\n",
    "The SpaCy library is an open-source software library for advanced natural language processing (NLP), written in Python and Cython. Unlike libraries such as NLTK, which are often used for teaching and research, spaCy is designed specifically for production use, offering fast and robust tools for tasks like tokenization, part-of-speech tagging, dependency parsing, text categorization, and named entity recognition (NER).\n",
    "\n",
    "spaCy supports deep learning workflows and can integrate with popular machine learning libraries such as TensorFlow and PyTorch via its own backend, Thinc. It provides prebuilt neural network models for 23 languages and supports tokenization for over 65 languages, making it suitable for both multilingual applications and custom model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-genai\n",
    "%pip install seqeval\n",
    "%pip install google-generativeai\n",
    "%pip install ipywidgets --upgrade\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydantic\n",
    "%pip install datasets[all]\n",
    "%pip install pandas\n",
    "%pip install ollama\n",
    "%pip install spacy\n",
    "%python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425123a",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For the development and evaluation of the prompts in this project, we used two different large language models: Gemma 3 (27B) and DeepSeek.\n",
    "\n",
    "## Gemma 3 (27b)\n",
    "\n",
    "We chose Gemma 3 because it is a recently released model by Google, offering several notable advantages:\n",
    "\n",
    "* Computational efficiency: Gemma 3 is a relatively lightweight model that achieves reasonable performance compared to state-of-the-art (SOTA) models like Gemini 2.5 Pro, which has over ten times more parameters.\n",
    "* API limits: Unlike many other models with free-tier APIs, Gemma 3 allows for a higher number of requests per day, making it more suitable for iterative development and testing.\n",
    "\n",
    "However, these advantages come with a significant trade-off:\n",
    "\n",
    "* Lower performance: In our tests, Gemma 3 underperformed compared to more advanced models. When evaluated against Gemini 2.0 Flash, we observed a performance gap of up to 20% in F1-score. Despite this, using higher-performing models was impractical due to API rate limitations, which would have required a considerable amount of additonal time to complete equivalent testing. \n",
    "\n",
    "The lower accuracy of Gemma 3 also influenced our implementation: model outputs required additional validation and post-processing. In some cases, responses did not adhere to the expected output format, necessitating extra checks and error handling in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613880d",
   "metadata": {},
   "source": [
    "## DeepSeek-R1 (distilled)\n",
    "In this section, we show and discuss our implementation on the DeepSeek-R1 model (distilled).\n",
    "\n",
    "### How to run DeepSeek-R1 locally\n",
    "Go to https://ollama.com, download and install ollama.\n",
    "Then, search for DeepSeek-R1's available models: https://ollama.com/library/deepseek-r1.\n",
    "Click on a model you wish to download and copy the command to run in the terminal (for instance: ollama run deepseek-r1:14b).\n",
    "\n",
    "\n",
    "### Model selection\n",
    "Our choice for the model was based on two factors: time consumption and performance.\n",
    "We decided to use `deepseek-r1:14b` because it generated tokens relatively fast while still performing almost as good as gemma-3-27b-it, a model we used previously which had acceptable performance.\n",
    "Trying to run bigger models such as deepseek-r1:32b was not feasible due to the very long token generation times.\n",
    "The increase in performance compared to deepseek-r1:14b was also very minimal, which led us to try even smaller models, such as deepseek-r1:7b and deepseek-r1:8b, which performed much worse instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87739fc",
   "metadata": {},
   "source": [
    "# **Implemented methods**\n",
    "In this section, we list and describe the methods we implemented using deepseek-r1:14b. Each prompt corresponds to a specific approach, and the results for each method are saved in a CSV file identified by the method's acronym.\n",
    "\n",
    "## - Word-level named entity reflection (WLNER)\n",
    "\n",
    "In this method, we add to the vanilla prompt another instruction which asks the model to generate a short summary for each word in the sentence. The summary had to be very short (around ten words) and is supposed to give an explanation, motivating the reason why a certain word can or cannot be a potential candidate to be classified as a NER tag.\n",
    "\n",
    "This approach explicitly leverages the **chain of thought** capability of large language models: by prompting the model to reflect and reason about each token individually, it is encouraged to articulate its decision-making process step by step. This not only helps the model to make more informed tagging decisions, but also provides interpretability, as the generated summaries reveal the rationale behind each prediction.\n",
    "\n",
    "However, this method performed slightly worse compared to the baseline reported in the reference paper.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "## - Multi-turn adaptive refinement (MTAR)\n",
    "\n",
    "In this method we asked the model to name the potential candidate words to be classified as NER tags. The model had to give an explanation for every word it believed to be a candidate NER tag, and then give the results in the correct format.\n",
    "\n",
    "Similarly to the previously mentioned method, this approach explicitly leverages the **chain of thought** capabilities of large language models. By prompting the model to reason step by step about which words are likely entities and to justify its choices, we encourage a more structured and interpretable decision-making process. This not only helps the model focus on the most relevant tokens, but also reduces the overall size of the output text generated, since the prompt specifies to just list the candidate NER tags and their explanations.\n",
    "\n",
    "This method performed slightly better compared to the baseline counterpart.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "## - Dependency-based entity validation (DBEV)\n",
    "In this method we added the dependency tree in the prompt, obtained by using spaCy (the dependency tree was not part of the dataset).\n",
    "\n",
    "The motivation behind this approach was to provide the model with additional syntactic information about the relationships between words in the sentence. By including the dependency tree, the model could potentially leverage the grammatical structure to better identify entities, especially in complex sentences where context and word dependencies play a crucial role.\n",
    "\n",
    "To implement this, we used spaCy to generate the dependency tree for each sentence, representing the syntactic relations in a structured format. This tree was then inserted into the prompt alongside the tokens to be tagged. The expectation was that the model, with access to this extra layer of linguistic information, would be able to make more informed tagging decisions, particularly in cases where surface-level cues were insufficient.\n",
    "\n",
    "However, in our experiments, we observed that the F1 score associated to this method was slightly worse.\n",
    "\n",
    "It is also important to note that the dependency trees were not originally part of the dataset and had to be computed using spaCy. This introduces a potential source of error, as the quality of the dependency parsing depends on the accuracy of spaCy's model. If the dataset had included gold-standard dependency trees, or if the dependency information had been annotated and curated specifically for the NER task, the results could have been better and the model might have been able to exploit this information more effectively.\n",
    "\n",
    ">This method was already used in the reference paper.\n",
    "\n",
    "## - POS-guided named entity recognition (POSGNER)\n",
    "In this method we added the POS tags for every token of the sentence, which are part of the dataset.\n",
    "\n",
    "The main idea behind this approach is to provide the model with explicit information about the grammatical role of each token in the sentence. Part-of-speech (POS) tags indicate whether a word is a noun, verb, adjective, proper noun, etc., and can be very helpful for named entity recognition because certain entity types are strongly associated with specific POS tags (for example, proper nouns are often persons, organizations, or locations).\n",
    "\n",
    "By including the POS tags in the prompt, we aimed to help the model disambiguate cases where the token alone might not be sufficient to determine the correct entity type. For instance, the word \"Apple\" could refer to a fruit (common noun) or a company (proper noun), and the POS tag can provide a useful clue for the model to make the right choice.\n",
    "\n",
    "In practice, we formatted the prompt so that the model received both the list of tokens and the corresponding POS tags in order. This additional information allowed the model to make more informed predictions, especially in sentences with ambiguous or rare words. Our experiments showed that this method generally improved the F1 score compared to the vanilla approach, confirming the usefulness of syntactic information for NER tasks.\n",
    "\n",
    "The performance scores indicate that this method was better than the baseline method implemented in the reference paper.\n",
    "\n",
    ">This method was already used in the reference paper.\n",
    "\n",
    "## - POS-dependency hybrid NER (POSDHNER)\n",
    "In this method we added to the prompt both the POS tags and the dependency tree.\n",
    "\n",
    "The rationale behind this approach is to combine two complementary sources of linguistic information: the part-of-speech (POS) tags, which indicate the grammatical role of each token, and the dependency tree, which describes the syntactic relationships between words in the sentence. By providing both types of annotations, we aimed to give the model a richer context for making NER predictions, especially in cases where either POS or dependency information alone might not be sufficient.\n",
    "\n",
    "In practice, the prompt was structured to include the list of tokens, their corresponding POS tags, and the full dependency tree (as computed by spaCy) for each sentence. This allowed the model to consider not only the type of each word but also how words are connected and which tokens are likely to form multi-word entities based on their syntactic structure.\n",
    "\n",
    "However, our experiments showed that the performance was close to the dependency-based entity validation method. Additionally, as with the dependency-based method, the quality of the dependency tree depends on the accuracy of spaCy's parser. If gold-standard dependency annotations had been available in the dataset, the results might have been better and the model could have leveraged this information more effectively.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "## - Example-driven POS NER (EDPOSNER)\n",
    "In this method we added the POS tags for every token of the sentence. We also added three complete examples, which consisted of the sentence tokens, POS tags and NER tags associated to such tokens. Thus this can be considered as a **few-shot learning** method.\n",
    "\n",
    "The main motivation for this approach was to provide the model with concrete, context-rich demonstrations of the NER task, making it easier for the model to generalize the tagging strategy to new sentences. By including three full examples in the prompt—each showing the tokens, their corresponding POS tags, and the correct NER tags—the model could observe how the tagging should be performed in practice, even for more complex or ambiguous cases.\n",
    "\n",
    "In our experiments, this method generally led to similar performance compared to the original prompt without examples.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project.\n",
    "\n",
    "## - POS NER with Viterbi Algorithm (POSVITNER)\n",
    "To compute POS tags, we experimented with prompting the model to use the Viterbi algorithm in conjunction with the English-specific Penn Treebank (PTB) tagset. The prompt included a complete list of BIO tags for the entity classes and enforced strict rules for the output format.\n",
    "\n",
    "We explored this approach to apply concepts covered during lectures. The underlying hypothesis was that requiring the model to first infer the POS tags would force it to reason more deeply about the syntactic structure of the sentence before performing NER. This additional layer of analysis was expected to improve overall tagging performance.\n",
    "\n",
    "Results showed the opposite: the method introduced significant computational overhead, and the performance was substantially worse compared to both the vanilla and best-performing approaches.\n",
    "\n",
    ">This method was designed and developed by us specifically for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79ecb",
   "metadata": {},
   "source": [
    "### Evaluation Without BIO Tag Distinction\n",
    "\n",
    "In addition to the standard evaluation using the full BIO tagging scheme (where each entity is labeled as either Beginning (B), Inside (I), or Outside (O)), we also conducted experiments by removing the distinction between B- and I- tags. In this alternative evaluation, entity types are considered without regard to whether a token is at the beginning or inside of an entity span. For example, both `B-PER` and `I-PER` are mapped to a single `PER` category.\n",
    "\n",
    "This approach allows us to consider a prediction as correct even if the model predicts a `B` tag instead of an `I` tag (or vice versa), as long as the entity type itself is correct. The motivation behind this evaluation is to focus on the model's ability to recognize the correct entity type, regardless of its position within the entity span. This can be particularly useful for assessing the robustness of the model in scenarios where the precise span boundaries are less critical than the correct identification of entity types.\n",
    "\n",
    "To implement this, we mapped all `B-` and `I-` tags of the same entity type to a single label. The evaluation metrics were then computed on these simplified labels, providing an additional perspective on the model's performance.\n",
    "\n",
    "The csvs obtained by ignoring the BIO tagging end with **NOBIO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2186b83",
   "metadata": {},
   "source": [
    "### Model's response parsers\n",
    "\n",
    "Here we list the functions for parsing and storing the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def parse_response(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        pred_label = int(response_labels[i].strip())\n",
    "        if(pred_label < 0 or pred_label > 8):\n",
    "            print(f\"Token: {tokens[i]}, Predicted Label: {pred_label}, True Label: {true_labels[i]}\")\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def save_to_csv_vanilla(tokens : list, pred_labels : list, true_labels : list, filename : str) -> None:\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # Write header only if the file didn't exist before\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    if (len(pred_labels) == 0 and len(true_labels) == 0):\n",
    "        return\n",
    "    data = [[tokens[i], pred_labels[i], true_labels[i]] for i in range(len(tokens)) if pred_labels[i] != 0 or true_labels[i] != 0]\n",
    "    # Remove duplicates\n",
    "    # Open the file in append mode and write data to analysis purpose\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "begin_tags = {1,3,5,7}\n",
    "\n",
    "def parse_response_no_bio(tokens : list, response_labels : list, true_labels : list) -> list: \n",
    "    '''\n",
    "    Store the response in a list of lists where the first element is the token, the second element \n",
    "    is the predicted label and the third is the true label\n",
    "    '''\n",
    "    response_labels = response_labels.split(\":\")\n",
    "    response_labels = response_labels[1].strip('\\n').split(',')\n",
    "    if (len(response_labels) != len(tokens)):\n",
    "        if (len(response_labels) > len(tokens)):\n",
    "            response_labels = response_labels[:len(tokens)]\n",
    "        if (len(response_labels) < len(tokens)):\n",
    "            response_labels = response_labels + ['0'] * (len(tokens) - len(response_labels))\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        if i > 0 and temp[i-1][1] in begin_tags and int(response_labels[i].strip()) in begin_tags:\n",
    "            # If the previous token is a begin tag and this is not '0', we assume it's a continuation\n",
    "            pred_label = int(response_labels[i].strip()) + 1\n",
    "        elif i > 0 and int(response_labels[i].strip()) in begin_tags and (temp[i-1][1] - 1) == int(response_labels[i].strip()):\n",
    "            # if previous token is an inside token and the current is in the same category, we assume it's a continuation\n",
    "            pred_label = int(response_labels[i].strip()) + 1\n",
    "        else:\n",
    "            # Otherwise, we take the label as is\n",
    "            pred_label = int(response_labels[i].strip())\n",
    "        if(pred_label < 0 or pred_label > 8):\n",
    "            print(f\"Token: {tokens[i]}, Predicted Label: {pred_label}, True Label: {true_labels[i]}\")\n",
    "        #assert (pred_label >= 0 and pred_label <= 8), \"Predicted label is out of range\"\n",
    "        temp.append([tokens[i], pred_label, true_labels[i]])\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e2353",
   "metadata": {},
   "source": [
    "### Training data sampling\n",
    "\n",
    "Here we sample the first 100 sentences from the training set using a fixed seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9648340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Sample 100 random elements from the test set\n",
    "sampled_train_data = random.sample(list(train_data), 100)\n",
    "\n",
    "# Print the first few samples to verify\n",
    "for i, sample in enumerate(sampled_train_data[:5]):  # Display the first 5 samples\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(sample)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f689085",
   "metadata": {},
   "source": [
    "### Dependency tree computation\n",
    "\n",
    "Here we report the code for computing the dependency trees for the training data usign spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "dependency_trees = []\n",
    "\n",
    "for sample in sampled_train_data:\n",
    "    tokens = sample[\"tokens\"]\n",
    "    sentence = \" \".join(tokens)\n",
    "    doc = nlp(sentence)\n",
    "    tree = [\n",
    "        {\n",
    "            \"text\": token.text,\n",
    "            \"dep\": token.dep_,\n",
    "            \"head\": token.head.text,\n",
    "            \"pos\": token.pos_,\n",
    "            \"index\": token.i,\n",
    "            \"head_index\": token.head.i\n",
    "        }\n",
    "        for token in doc\n",
    "    ]\n",
    "    dependency_trees.append(tree)\n",
    "\n",
    "# Link the two lists\n",
    "for i, sample in enumerate(sampled_train_data):\n",
    "    sample['dependency_tree'] = dependency_trees[i]\n",
    "\n",
    "# Now each sample has the tree inside\n",
    "print(sampled_train_data[0]['dependency_tree'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f014ccb",
   "metadata": {},
   "source": [
    "### Clean response function\n",
    "\n",
    "This function is used to format properly the response coming from a deepseek model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b679457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from ollama import Client\n",
    "\n",
    "\n",
    "def clean_response(text):\n",
    "    \"\"\"Cleans the model's output to extract only the numbers.\"\"\"\n",
    "    # Remove <think>...</think> blocks\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Look for the pattern 'ner_tags: 0, 1, 2, ...'\n",
    "    match = re.search(r'ner_tags\\s*:\\s*([0-9,\\s]+)', cleaned)\n",
    "    if match:\n",
    "        number_str = match.group(1)\n",
    "    else:\n",
    "        # If 'ner_tags:' is not found, but there are still numbers, extract them all\n",
    "        number_str = cleaned\n",
    "\n",
    "    # Extract all integers as strings\n",
    "    number_list = re.findall(r'\\d+', number_str)\n",
    "    return number_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5d1d1",
   "metadata": {},
   "source": [
    "### Format example function\n",
    "\n",
    "This function is used for the few-shot prompts, giving them a formatted example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a733e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(ex):\n",
    "    return f\"\"\"Tokens: {ex['tokens']}\n",
    "POS tags: {ex['pos_tags']}\n",
    "NER tags: {ex['ner_tags']}\"\"\"\n",
    "\n",
    "# Using 3 random examples in the prompt (different from the ones used in the training)\n",
    "example1 = train_data[11000]\n",
    "example2 = train_data[12000]\n",
    "example3 = train_data[13000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059385e",
   "metadata": {},
   "source": [
    "## POS-guided named entity recognition\n",
    "\n",
    "Here we report the best prompt found using deepseek-r1:14b and the code used to generate the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPSEEK\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    dependency_tree = dependency_trees[j]\n",
    "\n",
    "    # Convert dependency_tree to JSON string\n",
    "    dependency_tree_str = json.dumps(dependency_tree, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "    Given the following NER tags:\n",
    "    {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "    Your task is to assign the correct tag number to each token in this sentence:\n",
    "    {tokens}\n",
    "\n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Respond ONLY with:\n",
    "    ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "    Do NOT include explanations, thoughts, or any other content.\n",
    "    Do NOT write anything before or after \"ner_tags: ...\".\n",
    "    Just print the sequence in the format specified.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data100_train_ds/vanilla_train_100_ds_14b_POSGNER.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMMA\n",
    "from google import genai\n",
    "import time\n",
    "import os\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "    Given the following NER tags:\n",
    "    {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "    Your task is to assign the correct tag number to each token in this sentence:\n",
    "    {tokens}\n",
    "\n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Respond ONLY with:\n",
    "    ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "    Do NOT include explanations, thoughts, or any other content.\n",
    "    Do NOT write anything before or after \"ner_tags: ...\".\n",
    "    Just print the sequence in the format specified.\n",
    "    \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/pos_guided_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f30e37",
   "metadata": {},
   "source": [
    "### Using the same model without the BIO tagging\n",
    "\n",
    "Here we present the code used for the same prompt without using the BIO tagging.\n",
    "For this instance, the F1 score obtained using the nobio format was worse compared to his regular counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPSEEK\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    dependency_tree = dependency_trees[j]\n",
    "\n",
    "    # Convert dependency_tree to JSON string\n",
    "    dependency_tree_str = json.dumps(dependency_tree, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "    Given the following NER tags:\n",
    "    {{'O': 0, 'B-PER': 1, 'B-ORG': 3, 'B-LOC': 5, 'B-MISC': 7}}\n",
    "\n",
    "    Your task is to assign the correct tag number to each token in this sentence:\n",
    "    {tokens}\n",
    "\n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Respond ONLY with:\n",
    "    ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "    Do NOT include explanations, thoughts, or any other content.\n",
    "    Do NOT write anything before or after \"ner_tags: ...\".\n",
    "    Just print the sequence in the format specified.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response_no_bio(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data100_train_ds/vanilla_train_100_ds_14b_POSGNER_NOBIO.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4225ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMMA\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "    Given the following NER tags:\n",
    "    {{'O': 0, 'PER': 1, 'ORG': 3, 'LOC': 5, 'MISC': 7}}\n",
    "\n",
    "    Your task is to assign the correct tag number to each token in this sentence:\n",
    "    {tokens}\n",
    "\n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Respond ONLY with:\n",
    "    ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "    Do NOT include explanations, thoughts, or any other content.\n",
    "    Do NOT write anything before or after \"ner_tags: ...\".\n",
    "    Just print the sequence in the format specified.\n",
    "    \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/pos_guided_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc49d3",
   "metadata": {},
   "source": [
    "# Context Aware No BIO tagging Named Entity Recognition\n",
    "\n",
    "This is the prompt for Gemma3 that achieved the best performance on the training set. It does not use the BIO tagging format, as the model delivered better results without it during training. We hypothesize that omitting BIO tags reduces complexity for the model, allowing it to focus more on contextual understanding rather than token-level sequence formatting.\n",
    "\n",
    "While experimenting with more complex methods, such as longer prompts containing additional technical details, we observed no improvement in performance. In fact, providing excessive technical information appeared to hinder the model. Instead, we found that delivering clear, concise descriptions of the semantic context for each entity class and rules led to better outcomes.\n",
    "\n",
    "We also adopted this approach from a resource-efficiency perspective. Since Gemma3 is a relatively small model (27 billion parameters), a straightforward, tightly scoped prompt, which clearly defines the rules and includes only the essential information, requires less computation time compared to more elaborate methods like Decomposed QA, while still delivering consistent and reliable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMMA\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "            NER Tag IDs:\n",
    "            - 0 = O → Not an entity  \n",
    "            - 1 = PERSON → Real names, personal titles   \n",
    "            - 3 = ORGANIZATION → Companies, institutions, teams, agencies \n",
    "            - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "            - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works \n",
    "\n",
    "            ---\n",
    "\n",
    "            Rules:\n",
    "            - Use only contextual evidence to determine entity types.\n",
    "            - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "            - Please do not assign tags based on assumptions or incomplete context.\n",
    "            - Please be as precise as possible\n",
    "\n",
    "            ---\n",
    "\n",
    "            Now tag the sentence below:\n",
    "            Sentence: {tokens}  \n",
    "            This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "            Your answer MUST follow this format:  \n",
    "            ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "            (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "            \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/precision_enhanced_advanced_no_ex.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ece22a",
   "metadata": {},
   "source": [
    "## Decomposed QA\n",
    "\n",
    "We implemented this method based on the approach described in the paper. The idea is to reduce the complexity of the task by querying the model separately for each entity class. This allows the model to decompose the problem into smaller subproblems, solving each one while also leveraging its previous responses for context. \n",
    "Decomposing the task helped the model to generate higher quality responses, aside from occasional hallucinations where the output format was not followed. That said, the incidence of such errors was below 1% for Gemma when using the provided response parser.\n",
    "We implemented this approach only for Gemma because of the probitive computation time required from DeepSeek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c87bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "category_to_index = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, \n",
    "    'I-PER': 2, \n",
    "    'B-ORG': 3, \n",
    "    'I-ORG': 4, \n",
    "    'B-LOC': 5, \n",
    "    'I-LOC': 6, \n",
    "    'B-MISC': 7, \n",
    "    'I-MISC': 8\n",
    "    }\n",
    "\n",
    "def save_to_csv_qa(pred_labels, true_labels_indices, tokens, filename):\n",
    "    data = []\n",
    "    \n",
    "    # Convert true_labels_indices to a dictionary mapping tokens to their true labels\n",
    "    true_label_dict = {}\n",
    "    for token, label_idx in zip(tokens, true_labels_indices):\n",
    "        # Ensure we're using a valid label from label_mapping\n",
    "        if label_idx in label_mapping:\n",
    "            true_label_dict[token] = label_mapping[label_idx]\n",
    "        else:\n",
    "            print(f\"Warning: Unknown label index {label_idx} for token '{token}', defaulting to 'O'\")\n",
    "            true_label_dict[token] = 'O'\n",
    "    \n",
    "    for pred_label, tokens_str in pred_labels.items():\n",
    "        # Skip invalid labels\n",
    "        if pred_label == 'None' or pred_label not in category_to_index:\n",
    "            print(f\"Warning: Skipping invalid label '{pred_label}'\")\n",
    "            continue\n",
    "            \n",
    "        # Split by commas to get individual tokens\n",
    "        token_list = [t.strip() for t in tokens_str.split(',')]\n",
    "        \n",
    "        for token in token_list:\n",
    "            if not token:\n",
    "                continue\n",
    "                \n",
    "            # Check if token exists in original tokens\n",
    "            if token in tokens:\n",
    "                # Direct match with a single token\n",
    "                true_label = true_label_dict.get(token, 'O')\n",
    "                data.append([token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "            else:\n",
    "                # This could be a composite token or doesn't exist in our original tokens\n",
    "                # Try to match with individual tokens from the original list\n",
    "                matched_tokens = []\n",
    "                for orig_token in tokens:\n",
    "                    if token.strip().lower() == orig_token.strip().lower():\n",
    "                        true_label = true_label_dict.get(orig_token, 'O')\n",
    "                        data.append([orig_token, category_to_index[pred_label], category_to_index[true_label]])\n",
    "                        matched_tokens.append(orig_token)\n",
    "                \n",
    "                # If we still couldn't match it to any original tokens\n",
    "                if not matched_tokens:\n",
    "                    # Only add unknown tokens if they look reasonable (not empty, not \"None\", etc.)\n",
    "                    if token != \"None\" and len(token) > 1:\n",
    "                        # We'll add it with 'O' as the true label since we can't find it\n",
    "                        data.append([token, category_to_index[pred_label], category_to_index['O']])\n",
    "    \n",
    "    # Process any tokens that weren't in the predictions but have true labels\n",
    "    for token, true_label in true_label_dict.items():\n",
    "        if true_label != 'O':\n",
    "            # Check if this token was already processed\n",
    "            token_processed = any(entry[0] == token for entry in data)\n",
    "            if not token_processed:\n",
    "                data.append([token, category_to_index['O'], category_to_index[true_label]])\n",
    "    \n",
    "    # Write to CSV\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    if not file_exists:\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            header = ['token', 'pred', 'true']\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header)\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Function to parse model responses and extract predicted labels\n",
    "def parse_model_response(response_text):\n",
    "    results = {}\n",
    "    \n",
    "    if response_text == 'None':\n",
    "        return results\n",
    "        \n",
    "    # Split by & to get multiple label groups\n",
    "    answers = response_text.split('&')\n",
    "    answers = [answer.strip() for answer in answers]\n",
    "    \n",
    "    for answer in answers:\n",
    "        answer = answer.strip()\n",
    "        \n",
    "        # Skip empty answers or explicit \"None\" answers\n",
    "        if not answer or answer == 'None':\n",
    "            continue\n",
    "            \n",
    "        # Check if we have a proper format with colon\n",
    "        if ':' in answer:\n",
    "            parts = answer.split(':', 1)  # Split on first colon only\n",
    "            label = parts[0].strip().strip(\"'\")\n",
    "            \n",
    "            # Validate the label before adding\n",
    "            if label not in category_to_index:\n",
    "                print(f\"Warning: Skipping invalid label format: '{label}'\")\n",
    "                continue\n",
    "                \n",
    "            # Extract entities (everything after the colon)\n",
    "            entities = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            for known_label in category_to_index.keys():\n",
    "                if known_label + ':' in entities:\n",
    "                    entities = entities.replace(known_label + ':', '')\n",
    "\n",
    "            \n",
    "            # Only add if we have non-empty entities\n",
    "            if entities:\n",
    "                results[label] = entities\n",
    "        else:\n",
    "            print(f\"Warning: Invalid format in answer: '{answer}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "label_groups = [(1, 2), (3, 4), (5, 6), (7, 8)]\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemma-3-27b-it') \n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    results = {}\n",
    "    print(\"Tokens:\", tokens)\n",
    "    chat = model.start_chat()\n",
    "    # Send general instructions only at the beginning of the conversation\n",
    "    intro_message = (\n",
    "        f\"\"\"Given the following NER tags: {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}, determine the Named Entity Recognition (NER) tags for the following sentence.\n",
    "        If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "        \n",
    "        The sentence is: '{tokens}'.\n",
    "        This sentence contains exactly {len(tokens)} tokens.\n",
    "        Remember that a token can only be classified once.\"\"\"\n",
    "    )\n",
    "    # Send message to the chat\n",
    "    try:\n",
    "        chat.send_message(intro_message)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        # If the request fails, try again after 30 seconds to avoid rate limits\n",
    "        time.sleep(30)\n",
    "        chat.send_message(intro_message)\n",
    "\n",
    "    # Send a question for each label group in BIO format\n",
    "    for group in label_groups:\n",
    "        label_1 = label_mapping[group[0]]\n",
    "        label_2 = label_mapping[group[1]]\n",
    "        prompt = (\n",
    "            f\"\"\"Which are the tokens labeled as '{label_1}' and '{label_2}' in the text? \n",
    "            If there are multiple tokens for a single category then separate the list of tokens with commas.\n",
    "            The output must be in the format: B-PER: entity1, entity2, entity3 & I-PER: entity4, entity5.\n",
    "            If only one category is present, then the output should be: B-PER: entity1, entity2, entity3 or I-PER: entity4, entity5.\n",
    "            If both categories have no entities, just answer with 'None'.\"\"\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = chat.send_message(prompt)\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "            response = chat.send_message(prompt)\n",
    "\n",
    "        try:\n",
    "            results.update(parse_model_response(response.text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            continue\n",
    "    print(\"Response text:\", results)\n",
    "    # Save result to CSV with ad-hoc functin for Basic Decomposed-QA\n",
    "    save_to_csv_qa(results, true_labels, tokens, \"data_train_gemma/decomposed_qa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014ad2d",
   "metadata": {},
   "source": [
    "### Removal of Rows with Invalid Prediction Labels\n",
    "To ensure the correctness of our evaluation, we removed all rows from the CSV files where the prediction label (pred) was not an integer between 0 and 8 (inclusive). Specifically, any row with a pred value outside this range, or with a non-integer value, was discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "input_folder = 'data100_train_ds'\n",
    "output_folder = 'data100_ds_corrected'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        rows = []\n",
    "        with open(input_path, newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    pred = int(row['pred'])\n",
    "                    if 0 <= pred <= 8:\n",
    "                        row['pred'] = str(pred)\n",
    "                        rows.append(row)\n",
    "                    # Otherwise, the row is discarded\n",
    "                except ValueError:\n",
    "                    # Row is discarded if pred cannot be converted to int\n",
    "                    continue\n",
    "\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e870c7",
   "metadata": {},
   "source": [
    "## **Confrontation between our models, prompt switching and prompt mixing**\n",
    "After evaluating the various prompts for our models, we chose to focus on those approaches that provided the highest F1 scores.\n",
    "The next step consisted in applying the best-performing prompt from one model to the other model. In this way, we could directly compare whether the improvements were due to the prompt itself or to the specific model architecture.\n",
    "\n",
    "In particular, we took the prompt that gave the best results with the first model and used it as input for the second model, and vice versa. This allowed us to assess the transferability and general effectiveness of each prompt, independently of the model for which it was originally designed.\n",
    "\n",
    "In the end, we also experimented with combining the two best prompts, integrating their most effective elements into a single hybrid prompt. This \"prompt mixing\" approach aimed to leverage the strengths of both strategies, further improving the overall performance and robustness of our NER system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f918d",
   "metadata": {},
   "source": [
    "We now present the results obtained by running deepseek-r1:14b with the best prompt originally designed for Gemma 3-27b-it. The F1-score achieved with this approach is very close to the one obtained using the POS-guided named entity recognition prompt, suggesting that both prompts are effective for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c434997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPSEEK\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    dependency_tree = dependency_trees[j]\n",
    "\n",
    "    # Convert dependency_tree to JSON string\n",
    "    dependency_tree_str = json.dumps(dependency_tree, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "    NER Tag IDs:\n",
    "    - 0 = O → Not an entity  \n",
    "    - 1 = PERSON → Real names, personal titles\n",
    "    - 3 = ORGANIZATION → Companies, institutions, teams, agencies\n",
    "    - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "    - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works\n",
    "\n",
    "    ---\n",
    "\n",
    "    Rules:\n",
    "    - Use only contextual evidence to determine entity types.\n",
    "    - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "    - Please do not assign tags based on assumptions or incomplete context.\n",
    "    - Please be as precise as possible\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now tag the sentence below:\n",
    "    Sentence: {tokens}  \n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "    Your answer MUST follow this format:  \n",
    "    ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "    (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response_no_bio(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data100_train_ds/vanilla_train_100_ds_14b_ADVANCED_NOBIO.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c97d60",
   "metadata": {},
   "source": [
    "We now present the results obtained by running the model with a hybrid prompt that combines the best elements from the Gemma 3-27b-it prompt and the POS-guided named entity recognition approach from DeepSeek. In this prompt, the model is provided with both detailed definitions for each entity type and the part-of-speech (POS) tags for every token in the sentence. The instructions explicitly require the model to use only contextual evidence, avoid making assumptions, and to be as precise as possible when assigning entity tags.\n",
    "\n",
    "By integrating both the explicit entity schema and the syntactic information from POS tags, this prompt aims to maximize the model's ability to accurately identify and classify entities. The F1-score achieved with this combined prompt is even higher than those obtained with either the Gemma 3-27b-it prompt or the POS-guided prompt alone. This improvement suggests that the two strategies are complementary: the detailed entity definitions help clarify the categories, while the POS tags provide valuable grammatical context for disambiguating entity types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPSEEK\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "for j in range(len(sampled_train_data)):\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    dependency_tree = dependency_trees[j]\n",
    "\n",
    "    # Convert dependency_tree to JSON string\n",
    "    dependency_tree_str = json.dumps(dependency_tree, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "    NER Tag IDs:\n",
    "    - 0 = O → Not an entity  \n",
    "    - 1 = PERSON → Real names, personal titles\n",
    "    - 3 = ORGANIZATION → Companies, institutions, teams, agencies\n",
    "    - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "    - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works\n",
    "\n",
    "    ---\n",
    "\n",
    "    Rules:\n",
    "    - Use only contextual evidence to determine entity types.\n",
    "    - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "    - Please do not assign tags based on assumptions or incomplete context.\n",
    "    - Please be as precise as possible\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now tag the sentence below:\n",
    "    Sentence: {tokens}  \n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "    \n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Your answer MUST follow this format:  \n",
    "    ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "    (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response_no_bio(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data100_train_ds/vanilla_train_100_ds_14b_ADVANCEDPOS_NOBIO.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e65759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMMA\n",
    "for j in range(len(sampled_train_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_train_data[j]['tokens']\n",
    "    true_labels = sampled_train_data[j]['ner_tags']\n",
    "    pos_tags = sampled_train_data[j]['pos_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "    NER Tag IDs:\n",
    "    - 0 = O → Not an entity  \n",
    "    - 1 = PERSON → Real names, personal titles\n",
    "    - 3 = ORGANIZATION → Companies, institutions, teams, agencies\n",
    "    - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "    - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works\n",
    "\n",
    "    ---\n",
    "\n",
    "    Rules:\n",
    "    - Use only contextual evidence to determine entity types.\n",
    "    - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "    - Please do not assign tags based on assumptions or incomplete context.\n",
    "    - Please be as precise as possible\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now tag the sentence below:\n",
    "    Sentence: {tokens}  \n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "    \n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Your answer MUST follow this format:  \n",
    "    ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "    (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "    \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_train_gemma/pos_guided_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c118c",
   "metadata": {},
   "source": [
    "## Running the model on the test set\n",
    "\n",
    "In this section, we apply the best-performing prompt to the test set to evaluate the generalization ability of our models. This allows us to assess the final performance of each approach on unseen data and compare the results obtained during training with those on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "# Sample 100 random elements from the test set\n",
    "sampled_test_data = random.sample(list(test_data), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPSEEK\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    pos_tags = sampled_test_data[j]['pos_tags']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "    NER Tag IDs:\n",
    "    - 0 = O → Not an entity  \n",
    "    - 1 = PERSON → Real names, personal titles\n",
    "    - 3 = ORGANIZATION → Companies, institutions, teams, agencies\n",
    "    - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "    - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works\n",
    "\n",
    "    ---\n",
    "\n",
    "    Rules:\n",
    "    - Use only contextual evidence to determine entity types.\n",
    "    - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "    - Please do not assign tags based on assumptions or incomplete context.\n",
    "    - Please be as precise as possible\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now tag the sentence below:\n",
    "    Sentence: {tokens}  \n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "    \n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Your answer MUST follow this format:  \n",
    "    ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "    (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response_no_bio(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data100_test_ds/vanilla_test_100_ds_14b_ADVANCEDPOS_NOBIO.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26970201",
   "metadata": {},
   "source": [
    "### Using the vanilla prompt on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPSEEK\n",
    "# Initialize client\n",
    "client = Client()\n",
    "\n",
    "for j in range(len(sampled_test_data)):\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    pos_tags = sampled_test_data[j]['pos_tags']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"You are a strict NER tagging system.\n",
    "\n",
    "    Given the following NER tags:\n",
    "    {{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}}\n",
    "\n",
    "    Your task is to assign the correct tag number to each token in this sentence:\n",
    "    {tokens}\n",
    "\n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "\n",
    "    Respond ONLY with:\n",
    "    ner_tags: x, x, x, ..., x  ← (exactly {len(tokens)} integers)\n",
    "\n",
    "    Do NOT include explanations, thoughts, or any other content.\n",
    "    Do NOT write anything before or after \"ner_tags: ...\".\n",
    "    Just print the sequence in the format specified.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.generate(model=\"deepseek-r1:14b\", prompt=prompt)\n",
    "        raw_text = response.response\n",
    "        pred_tags_str = clean_response(raw_text)\n",
    "\n",
    "        # Parsing (conversion and validation)\n",
    "        parsed_data = parse_response(tokens, f\"ner_tags: {','.join(pred_tags_str)}\", true_labels)\n",
    "\n",
    "        # Debug print\n",
    "        print(f\"[✓] Sentence {j}\")\n",
    "        print(\"Tokens:    \", tokens)\n",
    "        print(\"Predicted: \", [x[1] for x in parsed_data])\n",
    "        print(\"True:      \", [x[2] for x in parsed_data])\n",
    "        print(\"---\")\n",
    "\n",
    "        # Save to file\n",
    "        save_to_csv_vanilla(tokens, [x[1] for x in parsed_data], true_labels, \"data100_test_ds/vanilla_test_100_ds_14b.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error at sentence {j}: {e}\")\n",
    "        print(f\"Raw response: {response.response if 'response' in locals() else 'None'}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMMA\n",
    "for j in range(len(sampled_test_data)):\n",
    "    # Extract tokens and true labels\n",
    "    tokens = sampled_test_data[j]['tokens']\n",
    "    true_labels = sampled_test_data[j]['ner_tags']\n",
    "    pos_tags = sampled_test_data[j]['pos_tags']\n",
    "    model = \"gemma-3-27b-it\"\n",
    "    #model = 'gemini-2.5-flash-preview-04-17'\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    content = f\"\"\"You are an expert in Named Entity Recognition (NER). Your task is to assign an entity tag ID to each token in the sentence using the strict schema below.\n",
    "\n",
    "    NER Tag IDs:\n",
    "    - 0 = O → Not an entity  \n",
    "    - 1 = PERSON → Real names, personal titles\n",
    "    - 3 = ORGANIZATION → Companies, institutions, teams, agencies\n",
    "    - 5 = LOCATION → Cities, countries, natural landmarks, buildings\n",
    "    - 7 = MISCELLANEOUS → Nationalities, languages, events, products, titles of works\n",
    "\n",
    "    ---\n",
    "\n",
    "    Rules:\n",
    "    - Use only contextual evidence to determine entity types.\n",
    "    - If the token does not clearly match a defined type, assign `0 (O)`.\n",
    "    - Please do not assign tags based on assumptions or incomplete context.\n",
    "    - Please be as precise as possible\n",
    "\n",
    "    ---\n",
    "\n",
    "    Now tag the sentence below:\n",
    "    Sentence: {tokens}  \n",
    "    This sentence contains exactly {len(tokens)} tokens.\n",
    "    \n",
    "    You are also given the POS tag (part-of-speech) for each token.\n",
    "    POS tags (in order, one per token):\n",
    "    {pos_tags}\n",
    "\n",
    "    Your answer MUST follow this format:  \n",
    "    ner_tags: 0, 1, 0, 5, 5, 0  \n",
    "    (Must return exactly {len(tokens)} tag IDs in order, no extra text.)\n",
    "    \"\"\"\n",
    "    # Send request \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents= content\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        time.sleep(5)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=content\n",
    "        )\n",
    "\n",
    "    # Parse the response\n",
    "    data = parse_response_no_bio(tokens, response.text, true_labels)\n",
    "\n",
    "    save_to_csv_vanilla(tokens, [item[1] for item in data], true_labels, \"data_test_gemma/pos_guided_no_bio.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827ee67",
   "metadata": {},
   "source": [
    "### Performance on test set\n",
    "Here we report our function used to give an in-depth summarization of the performance of the prompts.\n",
    "The results will be shown down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03287eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC',\n",
    "    7: 'B-MISC',\n",
    "    8: 'I-MISC'\n",
    "}\n",
    "\n",
    "category_to_index = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1, \n",
    "    'I-PER': 2, \n",
    "    'B-ORG': 3, \n",
    "    'I-ORG': 4, \n",
    "    'B-LOC': 5, \n",
    "    'I-LOC': 6, \n",
    "    'B-MISC': 7, \n",
    "    'I-MISC': 8\n",
    "    }\n",
    "\n",
    "valid_labels = set(label_mapping.values())\n",
    "\n",
    "def evaluate_predictions(filename: str) -> None:\n",
    "    true_seqs = []\n",
    "    pred_seqs = []\n",
    "    current_true = []\n",
    "    current_pred = []\n",
    "    invalid_count = 0\n",
    "\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            true_index = int(row['true'])\n",
    "            pred_index = int(row['pred'])\n",
    "            \n",
    "            true_label = label_mapping.get(true_index, 'O')\n",
    "            pred_label = label_mapping.get(pred_index, 'INVALID')\n",
    "\n",
    "            if pred_label not in valid_labels:\n",
    "                pred_label = 'INVALID'\n",
    "                invalid_count += 1\n",
    "                continue\n",
    "\n",
    "            current_true.append(true_label)\n",
    "            current_pred.append(pred_label)\n",
    "\n",
    "        true_seqs.append(current_true)\n",
    "        pred_seqs.append(current_pred)\n",
    "\n",
    "    print(f\"Invalid predictions: {invalid_count}\\n\")\n",
    "\n",
    "    print(\"Precision:\", precision_score(true_seqs, pred_seqs))\n",
    "    print(\"Recall:\", recall_score(true_seqs, pred_seqs))\n",
    "    print(\"F1 Score:\", f1_score(true_seqs, pred_seqs))\n",
    "\n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(classification_report(true_seqs, pred_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22612392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_ner_confusion_matrix_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Generates and visualizes a confusion matrix for NER data from a CSV file.\n",
    "\n",
    "    The CSV file is expected to have headers: 'token', 'pred', 'true'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing the data.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure the required columns exist\n",
    "        if 'pred' not in df.columns or 'true' not in df.columns:\n",
    "            print(\"Error: CSV file must contain 'pred' and 'true' columns.\")\n",
    "            return\n",
    "\n",
    "        # Extract predicted and true label indices\n",
    "        predicted_indices = df['pred'].tolist()\n",
    "        true_indices = df['true'].tolist()\n",
    "\n",
    "        # Convert indices to string labels using the mapping\n",
    "        predicted_labels_flat = [label_mapping.get(idx, 'UNKNOWN') for idx in predicted_indices]\n",
    "        true_labels_flat = [label_mapping.get(idx, 'UNKNOWN') for idx in true_indices]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get all unique labels present in the mapping to ensure all classes are in the matrix\n",
    "    all_possible_labels = sorted(list(label_mapping.values()))\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_labels_flat, predicted_labels_flat, labels=all_possible_labels)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=all_possible_labels,\n",
    "                yticklabels=all_possible_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix for NER Tags')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b53317",
   "metadata": {},
   "source": [
    "## DeepSeek's Results\n",
    "### Baseline prompt and best prompt comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d36339",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data100_test_ds/vanilla_test_100_ds_14b.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data100_test_ds/vanilla_test_100_ds_14b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1719a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data100_test_ds/vanilla_test_100_ds_14b_ADVANCEDPOS_NOBIO.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data100_test_ds/vanilla_test_100_ds_14b_ADVANCEDPOS_NOBIO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478f6c3",
   "metadata": {},
   "source": [
    "On the test set, the baseline (vanilla) method achieved an F1 score of **0.43**, with a precision of **0.37** and a recall of **0.52**. In contrast, our best method reached an F1 score of **0.49**, with a precision of **0.44** and a recall of **0.55**. This demonstrates a clear improvement in both precision and recall, resulting in a higher overall F1 score.\n",
    "\n",
    "While the performance on the test set is slightly lower than on the training set (as expected due to the increased difficulty of generalization), the advanced prompt maintains a significant advantage over the baseline. This confirms that the improvements are not due to overfitting, but rather to a better modeling of the NER task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edac83d",
   "metadata": {},
   "source": [
    "## Gemma's Results\n",
    "All the test were conducted using seed 0 and a sample of 100 elements from the test set.\n",
    "\n",
    "The vanilla method on the test set showed that the base approach from the paper has low performance, achieving an F1 score of 0.33. In contrast, the vanilla method with BIO tagging performed significantly better, reaching an F1 score of 0.49, which is close to the performance of our best methods.\n",
    "\n",
    "The POS-guided and context-aware prompts achieved slightly better results than the vanilla BIO method, with F1 scores of 0.51 and 0.50, respectively. Their combination yielded further improvement, reaching 0.53.\n",
    "\n",
    "Finally, the Decomposed QA method delivered the best performance on the test set, achieving an F1 score of 0.56.\n",
    "\n",
    "During the development and testing of these methods, we observed that the input data has a significant impact on model performance for the NER task. For example, during the training phase, the best prompt was the combination of the best prompts from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data_test_gemma/vanilla_bio.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data_test_gemma/vanilla_bio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cc147",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data_test_gemma/vanilla_no_bio.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data_test_gemma/vanilla_no_bio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data_test_gemma/decomposed_qa.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data_test_gemma/decomposed_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data_test_gemma/context_aware.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data_test_gemma/context_aware.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data_test_gemma/pos_guided_no_bio.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data_test_gemma/pos_guided_no_bio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions('data_test_gemma/combination.csv')\n",
    "generate_ner_confusion_matrix_from_csv('data_test_gemma/combination.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2aebc5",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df594bf",
   "metadata": {},
   "source": [
    "\n",
    "The experiments conducted in this project highlight the crucial role of prompt engineering and model selection in zero/few-shot Named Entity Recognition (NER) tasks. Our results show that carefully designed prompts, especially those that incorporate linguistic information such as POS tags and clear task instructions, consistently improve the quality of entity recognition.\n",
    "\n",
    "It is also evident that the choice of language model has a significant impact on performance. Different models respond differently to the same prompts: some benefit more from additional syntactic information or decomposed tasks, while others may perform better with concise, context-focused instructions. This variability suggests that there is no universally optimal prompt, but rather that prompt effectiveness depends on the specific characteristics and capabilities of the underlying model.\n",
    "\n",
    "It should also be noted that the datasets used for both training and evaluation were relatively limited in size. This constraint may have influenced the observed results and the generalization ability of the models. If a larger dataset had been used, the performance metrics and the relative effectiveness of different prompts and models could potentially vary, possibly revealing new strengths or weaknesses not captured in our current experiments.\n",
    "\n",
    "These findings confirm that both prompt design and model architecture must be considered together to maximize NER performance. Investing effort in prompt engineering and leveraging the strengths of each model can yield substantial improvements, even in zero-shot or few-shot scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
